{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BQlRqLF2ESV"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4 pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYKECoMzNu4A"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://clutch.co/us/real-estate/commercial-property-management'\n",
        "apikey = '50767f918a79d8aabf6c792d847cd11204636130'\n",
        "params = {\n",
        "    'url': url,\n",
        "    'apikey': apikey,\n",
        "}\n",
        "response = requests.get('https://api.zenrows.com/v1/', params=params)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqFKpbzge0TQ",
        "outputId": "2213b225-45e4-40d5-9ebe-b7bb4f4c62fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "✅ Saved zenrows_clutch_event_management_results.csv with 2108 entries.\n"
          ]
        }
      ],
      "source": [
        "# ✅ Google Colab-friendly Clutch Scraper using ZenRows (Updated to extract name + profile link)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "API_KEY = \"50767f918a79d8aabf6c792d847cd11204636130\"  # Replace this with your ZenRows API key\n",
        "\n",
        "company_names = []\n",
        "company_links = []\n",
        "base_url = \"https://clutch.co/us/pr-firms/event-management\"\n",
        "\n",
        "for page in range(0, 40):  # Scrape pages\n",
        "    if page == 0:\n",
        "        target_url = base_url  # first page\n",
        "    else:\n",
        "        print(f\"Scraping page {page}...\")\n",
        "        target_url = f\"{base_url}?page={page}\"\n",
        "\n",
        "    zenrows_url = f\"https://api.zenrows.com/v1/?apikey={API_KEY}&url={target_url}&js_render=true\"\n",
        "\n",
        "    res = requests.get(zenrows_url)\n",
        "\n",
        "    if res.status_code != 200:\n",
        "        print(f\"❌ Error {res.status_code} on page {page}\")\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    for provider in soup.select(\"div.provider__main-info a.directory_profile\"):\n",
        "        name = provider.get(\"title\", \"\").replace(\"See \", \"\").replace(\" Profile\", \"\").strip()\n",
        "        link = \"https://clutch.co\" + provider.get(\"href\", \"\").strip()\n",
        "\n",
        "        company_names.append(name)\n",
        "        company_links.append(link)\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame({\n",
        "    \"Company Name\": company_names,\n",
        "    \"Clutch Profile URL\": company_links\n",
        "})\n",
        "\n",
        "# Drop duplicates\n",
        "df = df.drop_duplicates(subset=[\"Company Name\"])\n",
        "df.to_csv(\"zenrows_clutch_event_management_results.csv\", index=False)\n",
        "\n",
        "print(\"✅ Saved zenrows_clutch_event_management_results.csv with\", len(df), \"entries.\")\n",
        "\n",
        "# Optional: download CSV directly in Colab\n",
        "# from google.colab import files\n",
        "# files.download(\"zenrows_clutch_results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaVJc88fH1b0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "df = pd.read_csv(\"zenrows_clutch_event_management_results.csv\")\n",
        "\n",
        "# Step 2: Remove rows where 'Company Name' ends with 'Reviews' (case-insensitive)\n",
        "df = df[~df[\"Company Name\"].str.lower().str.endswith(\"reviews\")]\n",
        "\n",
        "# Step 3: (Optional) Clean up any names that still contain trailing 'Reviews'\n",
        "df[\"Company Name\"] = df[\"Company Name\"].str.replace(r\"(?i)\\s+reviews$\", \"\", regex=True).str.strip()\n",
        "\n",
        "# Step 4: Save cleaned version\n",
        "df.to_csv(\"clutch_event_management_cleaned_results.csv\", index=False)\n",
        "\n",
        "# Step 5: Download in Google Colab (optional)\n",
        "# from google.colab import files\n",
        "# files.download(\"clutch_cleaned_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MUBZ728p0lv"
      },
      "source": [
        "Using Playwright to scrape data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-gN_0Djk3vs",
        "outputId": "6ae7c67c-fbca-40fe-a01c-a8797ff9b3a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.53.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting pyee<14,>=13 (from playwright)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<14,>=13->playwright) (4.14.1)\n",
            "Downloading playwright-1.53.0-py3-none-manylinux1_x86_64.whl (45.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.53.0 pyee-13.0.0\n",
            "Downloading Chromium 138.0.7204.23 (playwright build v1179)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1179/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G171.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 0% 20.9s\u001b[0K\u001b[1G171.6 MiB [] 0% 9.5s\u001b[0K\u001b[1G171.6 MiB [] 1% 4.7s\u001b[0K\u001b[1G171.6 MiB [] 1% 3.5s\u001b[0K\u001b[1G171.6 MiB [] 2% 2.8s\u001b[0K\u001b[1G171.6 MiB [] 3% 2.4s\u001b[0K\u001b[1G171.6 MiB [] 4% 2.3s\u001b[0K\u001b[1G171.6 MiB [] 5% 2.5s\u001b[0K\u001b[1G171.6 MiB [] 6% 2.3s\u001b[0K\u001b[1G171.6 MiB [] 7% 2.2s\u001b[0K\u001b[1G171.6 MiB [] 8% 2.1s\u001b[0K\u001b[1G171.6 MiB [] 9% 2.0s\u001b[0K\u001b[1G171.6 MiB [] 10% 2.0s\u001b[0K\u001b[1G171.6 MiB [] 11% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 12% 1.8s\u001b[0K\u001b[1G171.6 MiB [] 13% 1.8s\u001b[0K\u001b[1G171.6 MiB [] 14% 1.8s\u001b[0K\u001b[1G171.6 MiB [] 15% 1.7s\u001b[0K\u001b[1G171.6 MiB [] 16% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 18% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 19% 1.5s\u001b[0K\u001b[1G171.6 MiB [] 20% 1.5s\u001b[0K\u001b[1G171.6 MiB [] 21% 1.5s\u001b[0K\u001b[1G171.6 MiB [] 22% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 23% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 25% 1.3s\u001b[0K\u001b[1G171.6 MiB [] 26% 1.3s\u001b[0K\u001b[1G171.6 MiB [] 26% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 27% 1.3s\u001b[0K\u001b[1G171.6 MiB [] 29% 1.3s\u001b[0K\u001b[1G171.6 MiB [] 30% 1.2s\u001b[0K\u001b[1G171.6 MiB [] 32% 1.2s\u001b[0K\u001b[1G171.6 MiB [] 33% 1.1s\u001b[0K\u001b[1G171.6 MiB [] 35% 1.1s\u001b[0K\u001b[1G171.6 MiB [] 36% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 38% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 39% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 40% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 41% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 43% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 44% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 46% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 47% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 48% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 50% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 51% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 53% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 54% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 56% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 57% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 59% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 60% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 62% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 63% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 64% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 65% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 66% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 67% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 68% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 69% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 70% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 71% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 72% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 73% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 74% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 75% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 76% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 77% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 78% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 80% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 81% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 82% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 84% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 85% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 87% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 88% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 89% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 91% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 93% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 94% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 95% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 97% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 98% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 99% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 138.0.7204.23 (playwright build v1179) downloaded to /root/.cache/ms-playwright/chromium-1179\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 17% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 47% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 83% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Downloading Chromium Headless Shell 138.0.7204.23 (playwright build v1179)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1179/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G104.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 0% 18.2s\u001b[0K\u001b[1G104.5 MiB [] 0% 7.7s\u001b[0K\u001b[1G104.5 MiB [] 0% 4.9s\u001b[0K\u001b[1G104.5 MiB [] 1% 3.6s\u001b[0K\u001b[1G104.5 MiB [] 2% 3.0s\u001b[0K\u001b[1G104.5 MiB [] 3% 2.8s\u001b[0K\u001b[1G104.5 MiB [] 4% 2.6s\u001b[0K\u001b[1G104.5 MiB [] 4% 2.5s\u001b[0K\u001b[1G104.5 MiB [] 5% 2.4s\u001b[0K\u001b[1G104.5 MiB [] 6% 2.4s\u001b[0K\u001b[1G104.5 MiB [] 7% 2.3s\u001b[0K\u001b[1G104.5 MiB [] 8% 2.3s\u001b[0K\u001b[1G104.5 MiB [] 9% 2.4s\u001b[0K\u001b[1G104.5 MiB [] 10% 2.3s\u001b[0K\u001b[1G104.5 MiB [] 11% 2.3s\u001b[0K\u001b[1G104.5 MiB [] 12% 2.2s\u001b[0K\u001b[1G104.5 MiB [] 14% 2.1s\u001b[0K\u001b[1G104.5 MiB [] 15% 2.1s\u001b[0K\u001b[1G104.5 MiB [] 17% 2.0s\u001b[0K\u001b[1G104.5 MiB [] 18% 1.9s\u001b[0K\u001b[1G104.5 MiB [] 19% 1.9s\u001b[0K\u001b[1G104.5 MiB [] 20% 1.8s\u001b[0K\u001b[1G104.5 MiB [] 21% 1.7s\u001b[0K\u001b[1G104.5 MiB [] 22% 1.7s\u001b[0K\u001b[1G104.5 MiB [] 24% 1.6s\u001b[0K\u001b[1G104.5 MiB [] 25% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 26% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 27% 1.4s\u001b[0K\u001b[1G104.5 MiB [] 29% 1.4s\u001b[0K\u001b[1G104.5 MiB [] 30% 1.4s\u001b[0K\u001b[1G104.5 MiB [] 31% 1.4s\u001b[0K\u001b[1G104.5 MiB [] 32% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 33% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 34% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 36% 1.2s\u001b[0K\u001b[1G104.5 MiB [] 37% 1.2s\u001b[0K\u001b[1G104.5 MiB [] 39% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 40% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 41% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 42% 1.0s\u001b[0K\u001b[1G104.5 MiB [] 42% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 43% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 44% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 45% 1.0s\u001b[0K\u001b[1G104.5 MiB [] 46% 1.0s\u001b[0K\u001b[1G104.5 MiB [] 48% 0.9s\u001b[0K\u001b[1G104.5 MiB [] 49% 0.9s\u001b[0K\u001b[1G104.5 MiB [] 51% 0.9s\u001b[0K\u001b[1G104.5 MiB [] 54% 0.8s\u001b[0K\u001b[1G104.5 MiB [] 56% 0.7s\u001b[0K\u001b[1G104.5 MiB [] 58% 0.7s\u001b[0K\u001b[1G104.5 MiB [] 60% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 62% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 65% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 67% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 69% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 71% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 74% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 76% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 78% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 80% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 83% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 85% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 87% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 90% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 91% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 96% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 138.0.7204.23 (playwright build v1179) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1179\n"
          ]
        }
      ],
      "source": [
        "!pip install playwright\n",
        "!playwright install chromium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgXlVo6izlcV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKAfMc2Fq2dW",
        "outputId": "4431af0d-6665-4fde-e1e2-7e3f60e6b2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html><html lang=\"en-US\" dir=\"ltr\"><head><title>Just a moment...</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta name=\"robots\" content=\"noindex,nofollow\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><style>*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,\"Helvetica Neue\",Arial,\"Noto Sans\",sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\",\"Segoe UI Symbol\",\"Noto Color Emoji\"}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;padding-left:1.5rem;max-width:60rem}@media (width <= 720px){.main-content{margin-top:4rem}}.h2{line-height:2.25rem;font-size:1.5rem;font-weight:500}@media (width <= 720px){.h2{line-height:1.5rem;font-size:1.25rem}}#challenge-error-text{background-image:url(\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+\");background-repeat:no-repeat;background-size:contain;padding-left:34px}@media (prefers-color-scheme: dark){body{background-color:#222;color:#d9d9d9}}</style><meta http-equiv=\"refresh\" content=\"360\"><script src=\"/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=9605a8ea4d9f1846\"></script><style>*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,\"Helvetica Neue\",Arial,\"Noto Sans\",sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\",\"Segoe UI Symbol\",\"Noto Color Emoji\"}button{font-family:system-ui,-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,\"Helvetica Neue\",Arial,\"Noto Sans\",sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\",\"Segoe UI Symbol\",\"Noto Color Emoji\"}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}body.theme-dark{background-color:#222;color:#d9d9d9}body.theme-dark a{color:#fff}body.theme-dark a:hover{text-decoration:underline;color:#ee730a}body.theme-dark .lds-ring div{border-color:#999 rgba(0,0,0,0) rgba(0,0,0,0)}body.theme-dark .font-red{color:#b20f03}body.theme-dark .ctp-button{background-color:#4693ff;color:#1d1d1d}body.theme-dark #challenge-success-text{background-image:url(\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4\")}body.theme-dark #challenge-error-text{background-image:url(\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+\")}body.theme-light{background-color:#fff;color:#313131}body.theme-light a{color:#0051c3}body.theme-light a:hover{text-decoration:underline;color:#ee730a}body.theme-light .lds-ring div{border-color:#595959 rgba(0,0,0,0) rgba(0,0,0,0)}body.theme-light .font-red{color:#fc574a}body.theme-light .ctp-button{border-color:#003681;background-color:#003681;color:#fff}body.theme-light #challenge-success-text{background-image:url(\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=\")}body.theme-light #challenge-error-text{background-image:url(\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+\")}a{transition:color 150ms ease;background-color:rgba(0,0,0,0);text-decoration:none;color:#0051c3}a:hover{text-decoration:underline;color:#ee730a}.main-content{margin:8rem auto;padding-right:1.5rem;padding-left:1.5rem;width:100%;max-width:60rem}.main-content .loading-verifying{height:76.391px}.spacer{margin:2rem 0}.spacer-top{margin-top:4rem}.spacer-bottom{margin-bottom:2rem}.heading-favicon{margin-right:.5rem;width:2rem;height:2rem}@media (width <= 720px){.main-content{margin-top:4rem}.heading-favicon{width:1.5rem;height:1.5rem}}.main-wrapper{display:flex;flex:1;flex-direction:column;align-items:center}.font-red{color:#b20f03}.h1{line-height:3.75rem;font-size:2.5rem;font-weight:500}.h2{line-height:2.25rem;font-size:1.5rem;font-weight:500}.core-msg{line-height:2.25rem;font-size:1.5rem;font-weight:400}.body-text{line-height:1.25rem;font-size:1rem;font-weight:400}@media (width <= 720px){.h1{line-height:1.75rem;font-size:1.5rem}.h2{line-height:1.5rem;font-size:1.25rem}.core-msg{line-height:1.5rem;font-size:1rem}}#challenge-error-text{background-image:url(\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+\");background-repeat:no-repeat;background-size:contain;padding-left:34px}#challenge-success-text{background-image:url(\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=\");background-repeat:no-repeat;background-size:contain;padding-left:42px}.text-center{text-align:center}.ctp-button{transition-duration:200ms;transition-property:background-color,border-color,color;transition-timing-function:ease;margin:2rem 0;border:.063rem solid #0051c3;border-radius:.313rem;background-color:#0051c3;cursor:pointer;padding:.375rem 1rem;line-height:1.313rem;color:#fff;font-size:.875rem}.ctp-button:hover{border-color:#003681;background-color:#003681;cursor:pointer;color:#fff}.footer{margin:0 auto;padding-right:1.5rem;padding-left:1.5rem;width:100%;max-width:60rem;line-height:1.125rem;font-size:.75rem}.footer-inner{border-top:1px solid #d9d9d9;padding-top:1rem;padding-bottom:1rem}.clearfix::after{display:table;clear:both;content:\"\"}.clearfix .column{float:left;padding-right:1.5rem;width:50%}.diagnostic-wrapper{margin-bottom:.5rem}.footer .ray-id{text-align:center}.footer .ray-id code{font-family:monaco,courier,monospace}.core-msg,.zone-name-title{overflow-wrap:break-word}@media (width <= 720px){.diagnostic-wrapper{display:flex;flex-wrap:wrap;justify-content:center}.clearfix::after{display:initial;clear:none;text-align:center;content:none}.column{padding-bottom:2rem}.clearfix .column{float:none;padding:0;width:auto;word-break:keep-all}.zone-name-title{margin-bottom:1rem}}.loading-verifying{height:76.391px}.lds-ring{display:inline-block;position:relative;width:1.875rem;height:1.875rem}.lds-ring div{box-sizing:border-box;display:block;position:absolute;border:.3rem solid #595959;border-radius:50%;border-color:#313131 rgba(0,0,0,0) rgba(0,0,0,0);width:1.875rem;height:1.875rem;animation:lds-ring 1.2s cubic-bezier(.5, 0, .5, 1) infinite}.lds-ring div:nth-child(1){animation-delay:-.45s}.lds-ring div:nth-child(2){animation-delay:-.3s}.lds-ring div:nth-child(3){animation-delay:-.15s}@keyframes lds-ring{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}.rtl .heading-favicon{margin-right:0;margin-left:.5rem}.rtl #challenge-success-text{background-position:right;padding-right:42px;padding-left:0}.rtl #challenge-error-text{background-position:right;padding-right:34px;padding-left:0}.challenge-content .loading-verifying{height:76.391px}@media (prefers-color-scheme: dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{text-decoration:underline;color:#ee730a}body .lds-ring div{border-color:#999 rgba(0,0,0,0) rgba(0,0,0,0)}body .font-red{color:#b20f03}body .ctp-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4\")}body #challenge-error-text{background-image:url(\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+\")}}</style><script src=\"https://challenges.cloudflare.com/turnstile/v0/b/a19380bcf0f6/api.js?onload=yuKrj2&amp;render=explicit\" async=\"\" defer=\"\" crossorigin=\"anonymous\"></script></head><body><div class=\"main-wrapper\" role=\"main\"><div class=\"main-content\"><h1 class=\"zone-name-title h1\">clutch.co</h1><p id=\"ZcZKh3\" class=\"h2 spacer-bottom\">Verifying you are human. This may take a few seconds.</p><div id=\"WepX1\" style=\"display: grid;\"><div><div><input type=\"hidden\" name=\"cf-turnstile-response\" id=\"cf-chl-widget-qwofc_response\"></div></div></div><div id=\"DmJt7\" class=\"spacer loading-verifying\" style=\"display: none; visibility: hidden;\"><div class=\"lds-ring\"><div></div><div></div><div></div><div></div></div></div><div id=\"mrky6\" class=\"core-msg spacer spacer-top\">clutch.co needs to review the security of your connection before proceeding.</div><div id=\"PWlp0\" style=\"display: none;\"><div id=\"challenge-success-text\" class=\"h2\">Verification successful</div><div class=\"core-msg spacer\">Waiting for clutch.co to respond...</div></div><noscript><div class=\"h2\"><span id=\"challenge-error-text\">Enable JavaScript and cookies to continue</span></div></noscript></div></div><script>(function(){window._cf_chl_opt = {cvId: '3',cZone: 'clutch.co',cType: 'managed',cRay: '9605a8ea4d9f1846',cH: 'pd9q6X5ljARJOiLKfH7O9U9OZG7uWiV3pYJkBfpY2AA-1752712531-1.2.1.1-g9w82EoroWxAMw4A..P7dB.65MxFGzwegVHxCFG8O3Z4ulsBZY9XHfASRIdXaKo8',cUPMDTk:\"\\/profile\\/luxury-property-care?__cf_chl_tk=AsYIzGuyDue7E_EG4pjsKeDgLBtYuJudDcTYdYcEJmo-1752712531-1.0.1.1-BBiRh55c5jbjUt4cbVwtMWBEbDqxpy5Vp2uAxHTHNHI\",cFPWv: 'b',cITimeS: '1752712531',cTplC:0,cTplV:5,cTplB: 'cf',fa:\"\\/profile\\/luxury-property-care?__cf_chl_f_tk=AsYIzGuyDue7E_EG4pjsKeDgLBtYuJudDcTYdYcEJmo-1752712531-1.0.1.1-BBiRh55c5jbjUt4cbVwtMWBEbDqxpy5Vp2uAxHTHNHI\",md: 'hjqvFXrWmLZF.jTKRxIbU5USWO_maeVlFSgYkRD0Jd8-1752712531-1.2.1.1-rIDmB_MNSvXtqEtPbZbY8DMwPrjyhyuL5EoV5HoVtmEvAQcu.LQazzRS6K23lIxbh5LjSii2GlQij4xIEfb7AZ2Cu_B_t4UgzUCt7pot48uZuv2kbv4zy67PtYtfpMa6A85o5IU2wWPuxcDioDb3H4nEYzH8uAW_1szEM1DSUz65ix9Aa_41b6rh_Uf6Kq5hdqlaMM_pq9piHOzArYVPPpaf0SizjfNWO3pg_aHrJXz6oTA0PEOUX5mHsOPgGcjpBdcRxT2BpPqCq7QGmAbWYN1yCrhEQ8Xep0IRH_uZEsx9SWhfpppbXjIn8P4mWdwKy.a86KdbstRkHLVxdGAlkr8aFbbSd5r50N.4PNoD5Af0e2jnGJaE4XOAU3831RWUGVFfh.1yA_OEu8ZevhnE.j0OK9oH769ImCSp2MnEt9EtxWLNvEKsLYfekySnDxeXjdnxYzw4GLJiqXXBYQkN9fZ47FfBYObrk3fCbQYgcLYu68sb382E5zpelj5ZOZNBknBCP9nPjyCs6hRiOhMRYVWjPpy5ERR9hMw8J0QFyJ9il2vqjZ9l9c7uIa3G4je.iXEKtpVs0by3oVFk9MdYkV2eU6dYhrVxmsrA5PBAM0XKPBcr86489ASH0g2pRxrkM_nePRhKVAtU9Qu46D4bdxpvj7Aki1cFtiwC4epM61Ks6GdT0n9MFTXoKszSGx5G.y3Th017MBlwtAyuotu8AYgnC0.5JA9KMq1pElbY3uJ22ii_1WnBB51M_gN87a9DOYKWIfa5SdSjQkmsQniQvYUEYDOlX1poyMi852MhmG6m_tlPl50eRVa.M_mZawQzTJiSVvXUpkYWVuhQ5vCTqYjz8qf3S0HYbPHhuE9Vn2gRVt_WPE_RuGTgVLo6bZL1F77oHl9biCMqPfuSIL4dg.1XCDZrUTvcRAIcjw1dN6tP9ag6n9KBJQ_0pse8V09gUBiNxsNznjYc53IPtfVeLgP5jN6AFbNBv7wW_hOFxZit_s5HBJarNwEYzUptczyGYzsF43rg1ADRZmzBpMEPg941_C7W9NYhC5UWJtwoV6rbV34rhU5djl6NfEdYaYBD',mdrd: 'lEVk044VHrUAzNiW4X91y4Vs00LvanE.hTDZunNFr3o-1752712531-1.2.1.1-SuYBT9VQpHT0Qs1_xZQOmdk0IDF72_rXlmVYp221uzKTBrecU49BYQ1i5sRsqBtNhxprnkOjSs1bAFHl7b4PvnfidRGUVa4SyBrSmTMPsLbxOxYCOVm80QoFfEp6mjdarNH0j26IiXnT0dtYajIKA3_DPzQ5.4SaHNUawUY3jALXCEjmJqFLKcWRLty0JCHmDgj847VRvTtwCTFXXNtNFyieRmxds1.cf2U1.oBf2qfdxTiWJVPwHKgAa9JrGcqaa9ifY6hroK_fHHrP.KroPcsAwROO2ORPRu68UR72rc9.NrhrwuWxVuABpiAMHlMOsZp_q_0Sdm2kUq.nZ_s2pfuZ0.Gu8j1lSzIw0OqNll83Vf62GmodZ4jHVgmmsEdaYDKSgLU_ZJ3VEkv0JfVWVPGSS6kGuRtwmaaWjKbEANX2tehyS5895ngPwltr_GVN_wHMeOpQGnhPnUCdw1IJJAy.E1BAO.s7LUhI.H4qQkMCIJyoL4Xp8R5DNNKnC7XgwaklMY4ALc6LfKJzqrxNkrZ.a6huG4fqTAuHGtqbuGcjcpGXUiW.pRIPdzQo6WdbSksR81hqrCWOeywRjVyCfRCDUpWjmj2MuD20gahZGRVb7TD8hquQp8uAgSBjr8bcx8_yfusjESXH2REldjcioWUPGAcxB0h7v4Sp89E9HkLZR8M9I5aQsE8Rmp7CHcm5fNYCcXcvNMt1NSHF1ncsED3K_h_ecPdLtTlieBMnuxDVtfy09Lf8TUBvb9M.ZWGjMXx8wkmYUOUuLRZ9NpWWugBTvp3abunKIvd4qo.gFxfib79Ug1SD3lzzHIw89v.JOJyPrHYQ9Vh5rvWl7f7V0WBGsKsLQACOJyW32dzvf8vFaKh58_6FqVwxMNl7U87lr8W_.lMITF3u62vD2Rw2hd_ZX.m_hk5_1XqwuTfZE76ORUiZE8vrS9uYuICe1wi2JJGkegPLJgqO35X87a0V2K2VvjrnXOAnBZdG82fSbxXDt9RTI2bT6Iex34VfR7RtgIWz8k86Jv0xPCQn1x7n.MGVjhoXhSgNMeu4ePhsOHp1dTkl_81F3CyvnL7Kn8gIqqCVNeYrBYlPX6kLdzxfm9ktPXrp1wQfsE_Icf5LbM90mdPrIFyejDU_WNnH0VKTzHZue4ub8aF45q2HQc2Ps6h.HeUfD27hbd5mYZjAjROSp2vQWXIfjvokCmxx.ODQwaXKLqGWo88pzf02mgGTSwkqAWqWh5Q7JTS7DfTO9nb1SggtMdoqJ4W76_sXvcb94x2fltp4LpuTnDwln9pL_xECXwTpgO9M66qWDlSvHy57E5xwYsnSQdzZwAHsaDWX1YyOzd6Xf7CQTABAgN.c146jBFZZ7kz7ee4UQHFFEgWXppSZX3FtOWMDld4gC.LKw_RVVN6igaL5oGsqhJV_WPuwFWd8kUNrVYwsYXIKkzRffHkT69YjH9NVq45q6hbR6fkL3t8ALSSaLH4_TxwGzApU1OrS1Iz7f6NCsGxklMysR84BgfClXfzLidxaWQ7Hp5iZ3oyLaQAx5W7qLIhAHsPMxGjGVAfkfGJPCFE25RBteE7mxrZQuUNuRCmipvq0AbyL6zy0VTupEA2VGRkEnW8xG1uBfqpnSsKFwWvEiaV6UHGGRRs_4o5vWI8lXF_.nK_57DQdSeTZOTDoTk0oZHv5KeZD3dsQ4CVkNtVOZy3kuyB7th0VUoNk.zgMUBQ12l2sqDI5OltJi4nPtmFAGHJyI.cWtHDksUHHWck12Y_bgPNWnpMbEC6LK0rznA9AJxemoKmx5z8TILfFzMhKZHYf5s8Jt3suTE_1M142vOBKdtbAuCbqHG1Wt0xm0MsV9ySveeL0u3bXdJ_MtzXkjZ5mlG9ewMevZ3n3ltaURVKPSrNi8N3i7la8oCK.1baYaLO71bnHWS9wYsFzSPum_N5oDizZykRo9UXpoZTBvRfzpO3xOyspy5dh57Yf_zR6iwXOPHuNO4RlXBuIUL_uG07HUD8Dl.PsscyDUr2jtLyy9ADL6EMo5wmayrJogR29GPh2SGUG3c1ZAIfKrhKvw8atnf0fS_ZxZ47qwP_uEk4zrRFIEZn_VeLBXZUkgyF2ME5ZB.twy2zh9B3GJ_O9C4NcxucRYYVLbc6xVOfKWhfKj56CV9XhAZ8Wsg3JoB.C3HcXqg18sqAVek3610mjBfFEsRLnLh.WHa7o7K8zTJe41xcphNumNjsOfnqFsuWXXINE.Yauw3OXvM_b9p_bDgFOWhh7iJNiKwU6N2lDvpHWHbMvcE2tHW3O33bZ0nR7PaHNLdGUMb56RlZfrFPnQ_7RerctwfzxkQ2hSQPSszrHtafifv8nPGUNQJKyCRGQM7ZeuBGDbJyFLrdq9w.r5ubw8y64kHTz6E7sfz28VLzGobm8iDflbe8YGMfCFIoJHC8LwEpRT5vtzY_Nz_Jo4l.wiC8e_GvTq8cuF7EPPoTBP4x0WWsFdvJvi8XwAEJyOw2xfkoUKqpvgCq62KuGdnROeJbvjyAARaYQ9.uKiLe.en4TmHMTx_ZNoDAoHiqIaPt8ps39S0OSvZVzpTcbPfxBCw9Kkovjk1p6o7Jo9JfMDVuZVEpW.mN6D86KQUNKtOBEDKLcklMv5H_KnHZXLZaRq0IpA4j1CmnquIH5EEE',};var a = document.createElement('script');a.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=9605a8ea4d9f1846';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null,\"\\/profile\\/luxury-property-care?__cf_chl_rt_tk=AsYIzGuyDue7E_EG4pjsKeDgLBtYuJudDcTYdYcEJmo-1752712531-1.0.1.1-BBiRh55c5jbjUt4cbVwtMWBEbDqxpy5Vp2uAxHTHNHI\"+ window._cf_chl_opt.cOgUHash);a.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(a);}());</script><div class=\"footer\" role=\"contentinfo\"><div class=\"footer-inner\"><div class=\"clearfix diagnostic-wrapper\"><div class=\"ray-id\">Ray ID: <code>9605a8ea4d9f1846</code></div></div><div class=\"text-center\" id=\"footer-text\">Performance &amp; security by <a rel=\"noopener noreferrer\" href=\"https://www.cloudflare.com?utm_source=challenge&amp;utm_campaign=m\" target=\"_blank\">Cloudflare</a></div></div></div></body></html>\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def get_html_of_clutch_page():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        clutch_url = \"https://clutch.co/profile/luxury-property-care\"  # Replace with any URL\n",
        "        await page.goto(clutch_url, timeout=60000)\n",
        "\n",
        "        # Wait for JS to load (you can also wait for specific elements)\n",
        "        await page.wait_for_timeout(3000)\n",
        "\n",
        "        html = await page.content()\n",
        "        print(html[:100000])  # print first 3000 chars only to avoid Colab freezing\n",
        "        # with open(\"clutch_profile.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "        #   f.write(html)\n",
        "        await browser.close()\n",
        "\n",
        "await get_html_of_clutch_page()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtoc6dmH4GBl",
        "outputId": "d406cb50-63aa-4c07-8842-b50c455998f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Internet is working: 200\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "try:\n",
        "    res = requests.get(\"https://google.com\", timeout=10)\n",
        "    print(\"✅ Internet is working:\", res.status_code)\n",
        "except Exception as e:\n",
        "    print(\"❌ Internet issue:\", str(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzGqMevi1KYN",
        "outputId": "3e2b3067-707e-4cbb-a1be-8a9f032f1518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "    <meta charset=\"UTF-8\">\n",
            "\t\t<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n",
            "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
            "    <title>Luxury Property Care, 10 Reviews, Address, Data & More</title>    \n",
            "    <meta name=\"description\" content=\"At Luxury Property Care, we do more than just manage real estate. Luxury Property Care is a joint endeavor founded by Sivan Gerges and Liran Koren that brings\">\n",
            "    <meta http-equiv=\"x-dns-prefetch-control\" content=\"on\">\n",
            "    <meta name=\"robots\" content=\"index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1\"><link rel=\"canonical\" href=\"https://clutch.co/profile/luxury-property-care\">\n",
            "    \n",
            "    <meta property=\"og:title\" content=\"Luxury Property Care\">\n",
            "    <meta property=\"og:description\" content=\"At Luxury Property Care, we do more than just manage real estate. Luxury Property Care is a joint endeavor founded by Sivan Gerges and Liran Koren that brings\">\n",
            "    <meta property=\"og:image\" content=\"https://img.shgstatic.com/clutch-static-prod/og_profile/s3fs-public/logos/5f2552011e105aa13013e30323bae157_0.jpeg\">\n",
            "    <meta property=\"og:url\" content=\"https://clutch.co/profile/luxury-property-care\">\n",
            "    <meta property=\"og:type\" content=\"business.business\">\n",
            "    <meta property=\"business:contact_data:street_address\" content=\"950 Peninsula Corporate Cir 1013\">\n",
            "    <meta property=\"business:contact_data:locality\" content=\"Boca Raton\">\n",
            "    <meta property=\"business:contact_data:country_name\" content=\"United States\">\n",
            "    <meta property=\"twitter:site\" content=\"@clutch_co\">\n",
            "    <meta property=\"twitter:card\" content=\"summary_large_image\">\n",
            "    <meta property=\"twitter:title\" content=\"Luxury Property Care\">\n",
            "    <meta property=\"twitter:description\" content=\"At Luxury Property Care, we do more than just manage real estate. Luxury Property Care is a joint endeavor founded by Sivan Gerges and Liran Koren that brings\">\n",
            "    <link rel=\"shortcut icon\" href=\"/static/icons/favicon-white.png\" type=\"image/png\" />\n",
            "    <link rel=\"apple-touch-icon\" href=\"/static/icons/apple-touch-icon120x120.png\" type=\"application/octet-stream\"\n",
            "        sizes=\"120x120\" />\n",
            "    <link rel=\"apple-touch-icon\" href=\"/static/icons/apple-touch-icon152x152.png\" type=\"application/octet-stream\"\n",
            "        sizes=\"152x152\" />\n",
            "    <link rel=\"apple-touch-icon\" href=\"/static/icons/apple-touch-icon167x167.png\" type=\"application/octet-stream\"\n",
            "        sizes=\"167x167\" />\n",
            "    <link rel=\"apple-touch-icon\" href=\"/static/icons/apple-touch-icon180x180.png\" type=\"application/octet-stream\"\n",
            "        sizes=\"180x180\" />\n",
            "\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "\n",
            "\t\t\n",
            "\t\t<style id=\"inline_css\">\n",
            "\t    :root {\n",
            "\t      --fontDisplay: optional;\n",
            "\t    }\n",
            "\t    @font-face{font-display:var(--fontDisplay,optional);font-family:Roboto;font-style:normal;font-weight:300;src:local(\"Roboto Light\"),local(\"Roboto-Light\"),url(https://img.shgstatic.com/static/fonts/roboto-v19-latin-300.woff2) format(\"woff2\")}@font-face{font-display:var(--fontDisplay,optional);font-family:Roboto;font-style:normal;font-weight:400;src:local(\"Roboto\"),local(\"Roboto-Regular\"),url(https://img.shgstatic.com/static/fonts/roboto-v19-latin-regular.woff2) format(\"woff2\")}@font-face{font-display:var(--fontDisplay,optional);font-family:Roboto;font-style:normal;font-weight:600;src:local(\"Roboto Medium\"),local(\"Roboto-Medium\"),url(https://img.shgstatic.com/static/fonts/roboto-v20-latin-500.woff2) format(\"woff2\")}@font-face{font-display:var(--fontDisplay,optional);font-family:Roboto;font-style:normal;font-weight:700;src:local(\"Roboto Bold\"),local(\"Roboto-Bold\"),url(https://img.shgstatic.com/static/fonts/roboto-v20-latin-700.woff2) format(\"woff2\")}#common-header .sg-container{--sgContainerMobileCheckpoint:100%;--sgContainerTabletCheckpoint:100%;--sgContainerDesktopCheckpoint:1400px}#common-header .header__mobile-menu-button{background:transparent;border:0;height:48px;outline:0;padding:12px 6px;position:relative}@media(min-width:992px){#common-header .header__mobile-menu-button{content-visibility:hidden;display:none}}#common-header .header__mobile-menu-button:focus{outline:none}#common-header .header__mobile-menu-bar{background-color:#fff;display:block;height:3px;width:36px}#common-header .header__mobile-menu-bar+.header__mobile-menu-bar{margin-top:7px}#common-header .header__mobile-menu-bar:not(:last-child){transition:all .1s ease-in-out}#common-header .header__primary.header__primary--white .header__mobile-menu-bar{background-color:#4a5255}#common-header .header__primary.header__primary--white .search_mobile__button:after{background-color:#4a5255;mask-image:url('data:image/svg+xml;charset=utf-8,<svg xmlns=\"http://www.w3.org/2000/svg\" fill=\"none\" viewBox=\"0 0 14 15\"><path fill=\"%23fff\" d=\"M12.89 14.15 7.93 9.18a4.96 4.96 0 0 1-2.91 1c-1.3 0-2.4-.46-3.32-1.37a4.5 4.5 0 0 1-1.36-3.3c0-1.3.45-2.41 1.36-3.32A4.5 4.5 0 0 1 5.01.82c1.3 0 2.4.46 3.32 1.37A4.5 4.5 0 0 1 9.7 5.5a5 5 0 0 1-1 2.91l4.96 4.96zM5.02 9.1q1.5 0 2.55-1.05c.7-.7 1.04-1.55 1.04-2.55s-.35-1.85-1.04-2.55A3.47 3.47 0 0 0 5.02 1.9c-1 0-1.86.35-2.56 1.05S1.42 4.5 1.42 5.5s.35 1.85 1.04 2.55c.7.7 1.55 1.05 2.56 1.05\"/></svg>');mask-position:center;mask-repeat:no-repeat;mask-size:contain}#common-header .header__primary{background-color:#17313b;padding-bottom:8px;padding-top:8px}#common-header .header__primary-container{align-items:center;display:flex;gap:15px;justify-content:space-between;position:relative}#common-header .header__primary-logotype{display:flex}#common-header .header__primary-logotype-link{background-image:url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' xml:space='preserve' width='87.861' height='25'%3E%3Cpath fill='%23FFF' d='M22.861 0h4v25h-4zM40.861 17.025c0 3.826-3.217 4.131-4.174 4.131-2.391 0-2.826-2.238-2.826-3.588V8h-4v9.548c0 2.37.744 4.326 2.048 5.63 1.152 1.153 2.878 1.783 4.748 1.783 1.326 0 3.204-.413 4.204-1.326V25h4V8h-4zM52.861 2h-4v6h-3v4h3v13h4V12h3V8h-3zM68.458 19.917c-.871.783-2.021 1.217-3.283 1.217-2.782 0-4.825-2.043-4.825-4.848s1.978-4.762 4.825-4.762c1.24 0 2.412.413 3.305 1.196l.607.522 2.697-2.696-.675-.609C69.522 8.504 67.415 7.7 65.174 7.7c-5 0-8.631 3.608-8.631 8.565 0 4.936 3.718 8.673 8.631 8.673 2.283 0 4.412-.804 5.979-2.26l.652-.609-2.739-2.694zM86.061 9.482C84.909 8.33 83.559 7.7 81.689 7.7c-1.326 0-2.828.413-3.828 1.325V0h-4v25h4v-9.365c0-3.826 2.718-4.13 3.675-4.13 2.391 0 2.325 2.239 2.325 3.587V25h4v-9.887c0-2.37-.495-4.326-1.8-5.631'/%3E%3Cpath fill='%23E62415' d='M65.043 13.438a2.891 2.891 0 1 1 0 5.784 2.891 2.891 0 0 1 0-5.784'/%3E%3Cpath fill='%23FFF' d='M17.261 18.721c-1.521 1.565-3.587 2.413-5.761 2.413-4.456 0-7.696-3.5-7.696-8.304 0-4.826 3.24-8.326 7.696-8.326 2.153 0 4.196.847 5.74 2.391l.608.609 2.674-2.674-.587-.609C17.718 1.938 14.718.7 11.5.7 4.935.7 0 5.917 0 12.851 0 19.764 4.957 24.96 11.5 24.96c3.24 0 6.24-1.26 8.457-3.543l.587-.609-2.652-2.717z'/%3E%3C/svg%3E\");background-position:50%;background-repeat:no-repeat;background-size:contain;display:inline-block;height:36px;width:130px}#common-header .header__primary.header__primary--white{background-color:#fff;border-bottom:1px solid #e9e9e9}#common-header .header__primary.header__primary--white .header__primary-logotype-link{background-image:url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 405.7 115'%3E%3Ccircle cx='299.2' cy='75.3' r='13.3' fill='%23E62415'/%3E%3Cpath fill='%2317313B' d='M105.6 0h17.6v113.5h-17.6zm81.7 78.5c0 17.6-14.4 19-18.8 19-11 0-12.6-10.3-12.6-16.5V37.1h-17.7v43.8c-.1 10.9 3 19.9 9 25.9a32.1 32.1 0 0 0 40.1 2.1v4.6H205V37.1h-17.7zm55.9-67.1h-17.7v25.7h-12.3v16.7h12.3v59.7h17.7V53.8h14.5V37.1h-14.5zM315 91.8c-4 3.6-9.3 5.6-15.1 5.6a21.6 21.6 0 0 1-22.2-22.3c0-12.9 9.1-21.9 22.2-21.9 5.7 0 11.1 1.9 15.2 5.5l2.8 2.4 12.4-12.4-3.1-2.8a40.6 40.6 0 0 0-27.3-10.3c-23 0-39.7 16.6-39.7 39.4a39 39 0 0 0 39.7 39.9c10.5 0 20.3-3.7 27.5-10.4l3-2.8-12.6-12.4zm81.7-48a32.1 32.1 0 0 0-40.1-2.1V0h-17.7v113.5h17.7V72.2c0-17.6 14.4-19 18.8-19 11 0 12.6 10.3 12.6 16.5v43.9h17.7V69.7a36 36 0 0 0-9-25.9M79.5 86.3A36.7 36.7 0 0 1 53 97.4c-20.5 0-35.4-16.1-35.4-38.2C17.6 37 32.5 20.9 53 20.9c9.9 0 19.3 3.9 26.4 11l2.8 2.8 12.3-12.3-2.7-2.8A54.2 54.2 0 0 0 52.9 3.4C22.8 3.4 0 27.4 0 59.3 0 91 22.8 115 52.9 115c14.9 0 28.7-5.8 38.9-16.3l2.7-2.8-12.2-12.5z'/%3E%3C/svg%3E\");background-position:50%;background-repeat:no-repeat}@media(max-width:991px){#common-header .search_mobile__button{background:transparent;border:0;display:block;outline:none;padding:6px}#common-header .search_mobile__button:after{background-color:#fff;content:\"\";display:block;height:36px;mask-image:url('data:image/svg+xml;charset=utf-8,<svg xmlns=\"http://www.w3.org/2000/svg\" fill=\"none\" viewBox=\"0 0 14 15\"><path fill=\"%23fff\" d=\"M12.89 14.15 7.93 9.18a4.96 4.96 0 0 1-2.91 1c-1.3 0-2.4-.46-3.32-1.37a4.5 4.5 0 0 1-1.36-3.3c0-1.3.45-2.41 1.36-3.32A4.5 4.5 0 0 1 5.01.82c1.3 0 2.4.46 3.32 1.37A4.5 4.5 0 0 1 9.7 5.5a5 5 0 0 1-1 2.91l4.96 4.96zM5.02 9.1q1.5 0 2.55-1.05c.7-.7 1.04-1.55 1.04-2.55s-.35-1.85-1.04-2.55A3.47 3.47 0 0 0 5.02 1.9c-1 0-1.86.35-2.56 1.05S1.42 4.5 1.42 5.5s.35 1.85 1.04 2.55c.7.7 1.55 1.05 2.56 1.05\"/></svg>');mask-position:center;mask-repeat:no-repeat;mask-size:contain;width:36px}#common-header #service-menu.header__primary-list{content-visibility:hidden;display:none}}.header__primary{border-bottom:1px solid #4a5255}@media(max-width:991px){.header__secondary{content-visibility:hidden;display:none}}.sg-one-time-tooltip.hidden{display:none}.header #menu_search__form{display:flex;left:-100%;position:absolute;top:-1000%}.header__mobile-menu-container{display:none}\n",
            "\t\t</style>\n",
            "\n",
            "    \n",
            "    <link rel=\"stylesheet\" href=\"/static/css/cannon/cannon.f89de7ecd2.css\">\n",
            "    \n",
            "    <link rel=\"stylesheet\" href=\"/static/css/profile2/profile2.e2df8330c1.css\">\n",
            "    \n",
            "\n",
            "\n",
            "\t\t\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    <script async type=\"text/javascript\" nonce=\"bTCapHsBEtSqZjqS\">\n",
            "        window.MSG_API_URL = \"https://msg.clutch.co\";\n",
            "        window.MSG_API\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer 279163260297a4485cabc3edc036ab531e7e42d265e1d3ae0bb57b71fc35dcd5\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"zone\": \"clutch\",  # your Web Unlocker zone name\n",
        "    \"url\": \"https://clutch.co/profile/luxury-property-care\",  # target clutch profile\n",
        "    \"format\": \"raw\"  # so you get HTML directly\n",
        "}\n",
        "\n",
        "response = requests.post(\"https://api.brightdata.com/request\", json=data, headers=headers)\n",
        "\n",
        "# Print result\n",
        "print(response.status_code)\n",
        "print(response.text[:10000])  # preview first 2000 chars of HTML\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzUTzbpU7fSR",
        "outputId": "345b7a64-d414-44f6-c175-396b4e144ee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Real Business Website URL: https://luxurypropertycare.com/\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, parse_qs, unquote\n",
        "\n",
        "html = response.text\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "visit_links = soup.select(\"a.website-link__item\")\n",
        "\n",
        "real_url = None\n",
        "for tag in visit_links:\n",
        "    href = tag.get(\"href\")\n",
        "    if \"u=\" in href:\n",
        "        encoded_url = parse_qs(urlparse(href).query).get(\"u\", [\"\"])[0]\n",
        "        real_url = unquote(encoded_url)\n",
        "        break\n",
        "\n",
        "print(\"✅ Real Business Website URL:\", real_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nszrigYZ7as",
        "outputId": "3829a750-ecc0-446d-cc95-2d849a157b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Emails found: ['outreach@luxurypropertycare.com', 'outreach@lirankoren.com']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "API_KEY = \"3a6ea3c3-d7b4-458e-ab9c-86c72c662b16\"  # replace with your real one\n",
        "ZONE = \"clutch\"  # your zone name\n",
        "\n",
        "def extract_emails_from_html(html):\n",
        "    email_pattern = r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\"\n",
        "    return list(set(re.findall(email_pattern, html)))\n",
        "\n",
        "def get_website_html(url):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"zone\": ZONE,\n",
        "        \"url\": url,\n",
        "        \"format\": \"raw\"\n",
        "    }\n",
        "    res = requests.post(\"https://api.brightdata.com/request\", headers=headers, json=data)\n",
        "    if res.status_code == 200:\n",
        "        return res.text\n",
        "    else:\n",
        "        print(f\"Failed to fetch {url}: {res.status_code}\")\n",
        "        return \"\"\n",
        "\n",
        "# Example business website\n",
        "business_website = \"https://luxurypropertycare.com\"\n",
        "\n",
        "html = get_website_html(business_website)\n",
        "emails = extract_emails_from_html(html)\n",
        "\n",
        "print(\"✅ Emails found:\", emails)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kgMa1gChGfH",
        "outputId": "0d2a7492-b2b1-4d1a-a816-5b6cc4d3d5c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method DataFrame.count of                             Company Name  \\\n",
            "0                   Luxury Property Care   \n",
            "1                    Nomadic Real Estate   \n",
            "2     Four Star General Cleaning Service   \n",
            "3         Icon Real Estate Services, Inc   \n",
            "4            Urban Street Ventures, Inc.   \n",
            "...                                  ...   \n",
            "7242                   Workspace Dickson   \n",
            "7243          Workspace at Bishops Woods   \n",
            "7244      W.M. Corbin Construction Corp.   \n",
            "7245          Trade-Mark Industrial Inc.   \n",
            "7246            Crowns Field Association   \n",
            "\n",
            "                                     Clutch Profile URL  \n",
            "0        https://clutch.co/profile/luxury-property-care  \n",
            "1         https://clutch.co/profile/nomadic-real-estate  \n",
            "2     https://clutch.co/profile/four-star-general-cl...  \n",
            "3     https://clutch.co/profile/icon-real-estate-ser...  \n",
            "4       https://clutch.co/profile/urban-street-ventures  \n",
            "...                                                 ...  \n",
            "7242        https://clutch.co/profile/workspace-dickson  \n",
            "7243  https://clutch.co/profile/workspace-bishops-woods  \n",
            "7244  https://clutch.co/profile/wm-corbin-constructi...  \n",
            "7245    https://clutch.co/profile/trade-mark-industrial  \n",
            "7246  https://clutch.co/profile/crowns-field-associa...  \n",
            "\n",
            "[7247 rows x 2 columns]>\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/small-medium property management companies.csv\")  # Replace with your actual CSV filename\n",
        "print(df.count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcA6W0lLfBU5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, parse_qs, unquote\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Configuration\n",
        "API_KEY = \"3a6ea3c3-d7b4-458e-ab9c-86c72c662b16\"     # Replace with your API key\n",
        "ZONE = \"clutch\"                 # Replace with your BrightData zone name\n",
        "INPUT_FILE = \"/content/small-medium property management companies.csv\"      # Replace with your file\n",
        "BATCH_SIZE = 500\n",
        "NUM_WORKERS = 50                        # Number of threads (tweak if you hit rate limits)\n",
        "\n",
        "def fetch_and_extract(url):\n",
        "    try:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        data = {\n",
        "            \"zone\": ZONE,\n",
        "            \"url\": url,\n",
        "            \"format\": \"raw\"\n",
        "        }\n",
        "        res = requests.post(\"https://api.brightdata.com/request\", json=data, headers=headers, timeout=30)\n",
        "        if res.status_code != 200:\n",
        "            return url, \"\"\n",
        "\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "        link_tag = soup.select_one(\"a.website-link__item\")\n",
        "        if link_tag and \"href\" in link_tag.attrs:\n",
        "            parsed = parse_qs(urlparse(link_tag[\"href\"]).query)\n",
        "            if \"u\" in parsed:\n",
        "                return url, unquote(parsed[\"u\"][0])\n",
        "        return url, \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {e}\")\n",
        "        return url, \"\"\n",
        "\n",
        "# Load the full dataset\n",
        "df_full = pd.read_csv(INPUT_FILE)\n",
        "\n",
        "# Loop over batches\n",
        "for batch_idx in range(0, len(df_full), BATCH_SIZE):\n",
        "    batch_df = df_full.iloc[batch_idx:batch_idx + BATCH_SIZE].copy()\n",
        "    clutch_urls = batch_df[\"Clutch Profile URL\"].tolist()\n",
        "\n",
        "    print(f\"\\n🚀 Processing batch {batch_idx // BATCH_SIZE + 1} ({len(clutch_urls)} URLs)...\")\n",
        "\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
        "        future_to_url = {executor.submit(fetch_and_extract, url): url for url in clutch_urls}\n",
        "        for future in as_completed(future_to_url):\n",
        "            results.append(future.result())\n",
        "\n",
        "    url_to_website = dict(results)\n",
        "    batch_df[\"Website URL\"] = batch_df[\"Clutch Profile URL\"].map(url_to_website)\n",
        "\n",
        "    # Save batch output\n",
        "    output_file = f\"companies_with_websites_batch_{batch_idx // BATCH_SIZE + 1}.csv\"\n",
        "    batch_df.to_csv(output_file, index=False)\n",
        "    print(f\"✅ Saved {output_file}\")\n",
        "\n",
        "    # Optional: Pause briefly to avoid aggressive rate limits\n",
        "    time.sleep(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpqEVwmK8vkI",
        "outputId": "98dd6cd3-99fe-4913-d8ff-facfe6512874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Merged 15 files into companies_with_websites_full.csv\n"
          ]
        }
      ],
      "source": [
        "# Python file to merge all csv files\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern to match all batch files\n",
        "batch_files = sorted(glob.glob(\"companies_with_websites_batch_*.csv\"))\n",
        "\n",
        "# Combine all into one DataFrame\n",
        "df_all = pd.concat([pd.read_csv(file) for file in batch_files], ignore_index=True)\n",
        "\n",
        "# Optional: drop duplicates if needed\n",
        "# df_all = df_all.drop_duplicates(subset=[\"Clutch Profile URL\"])\n",
        "\n",
        "# Save to a final file\n",
        "df_all.to_csv(\"companies_with_websites_full.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Merged {len(batch_files)} files into companies_with_websites_full.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvHaQbzi-Gu8",
        "outputId": "f6b95a91-74df-4e93-8447-9a63d3663670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Done. File saved as test_brightdata_emails_fallback.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# Load CSV and get first 5 rows with valid website URLs\n",
        "df = pd.read_csv(\"companies_with_websites_full.csv\").dropna(subset=[\"Website URL\"]).head(20)\n",
        "\n",
        "# Bright Data config\n",
        "BRIGHTDATA_API_KEY = \"3a6ea3c3-d7b4-458e-ab9c-86c72c662b16\"  # Replace this\n",
        "ZONE = \"clutch\"              # Replace this\n",
        "ENDPOINT = \"https://api.brightdata.com/request\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {BRIGHTDATA_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Email regex\n",
        "def extract_emails(html):\n",
        "    emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.(?:com|org|net|edu)\", html)\n",
        "    clean = [e for e in emails if not e.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "    return list(set(clean))\n",
        "\n",
        "# BrightData scraper function\n",
        "def fetch_html(url):\n",
        "    try:\n",
        "        payload = {\n",
        "            \"url\": url,\n",
        "            \"zone\": ZONE,\n",
        "            \"format\": \"raw\"\n",
        "        }\n",
        "        res = requests.post(ENDPOINT, headers=headers, json=payload, timeout=30)\n",
        "        return res.text if res.status_code == 200 else \"\"\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# Try to find a contact page URL\n",
        "def find_contact_page(html, base_url):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    base = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(base_url))\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a['href'].lower()\n",
        "        if any(kw in href for kw in [\"contact\", \"contact-us\", \"contactus\"]) and not any(skip in href for skip in [\"mailto:\", \".jpg\", \".png\", \".pdf\", \"facebook\", \"instagram\", \"linkedin\"]):\n",
        "            return urljoin(base, href)\n",
        "    return None\n",
        "\n",
        "# Main loop\n",
        "email_list = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    website = row[\"Website URL\"]\n",
        "    email = \"\"\n",
        "\n",
        "    # Step 1: Scrape homepage\n",
        "    homepage_html = fetch_html(website)\n",
        "    homepage_emails = extract_emails(homepage_html)\n",
        "\n",
        "    if homepage_emails:\n",
        "        email = homepage_emails[0]\n",
        "    else:\n",
        "        # Step 2: Try contact page\n",
        "        contact_url = find_contact_page(homepage_html, website)\n",
        "        if contact_url:\n",
        "            contact_html = fetch_html(contact_url)\n",
        "            contact_emails = extract_emails(contact_html)\n",
        "            if contact_emails:\n",
        "                email = contact_emails[0]\n",
        "\n",
        "    email_list.append(email)\n",
        "\n",
        "# Add to DataFrame and save\n",
        "df[\"email_1\"] = email_list\n",
        "df.to_csv(\"test_brightdata_emails_fallback.csv\", index=False)\n",
        "print(\"✅ Done. File saved as test_brightdata_emails_fallback.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO5uR-pd-x9D",
        "outputId": "3273277d-0646-408f-f78f-31547852c32f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ Error fetching http://www.flynnco.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.t3advisors.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://lahvac.expert: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching http://www.weichertcommercial.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://ideapaintingboston.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.dataxivi.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://miamimoldspecialist.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.sandiegosecurityguards.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://ujsinc.com: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://magnetlocksmith.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.readycleantn.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.uicchicagoland.com: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching http://www.jdsplumbingservice.com: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://wattsac.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://atlanticcleaningco.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.miami-247locksmith.com: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://betahomebuyers.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://edwardsairenterprise.com: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.thermodynamixllc.com: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://platinumstarservices.com/house-cleaning-maid-services-allentown-pa/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.slon.org: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.tallpinepropertiesnh.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching http://comblepm.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching http://www.bearfootventures.com: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.crjlandscaping.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://www.lewiselectricservices.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://powerwashingatlanta.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching http://hmo-pestcontrol.com/contact-us/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://pleasantduct.cbscleaningus.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching http://www.besthvacservicerepair.com/contact.html: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://professionalpaintingtn.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://houstontxlocksmith.net/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching http://www.mcpherson1031dst.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching http://firstcallplumbersnj.com: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://groutworksdallas.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://scrubamerica.com/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "⚠️ Error fetching https://georgiamarbleservices.com/marble-polishing/: HTTPSConnectionPool(host='api.brightdata.com', port=443): Read timed out. (read timeout=10)\n",
            "✅ Saved batch 1 to email_batches/emails_batch_1.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import os\n",
        "import time\n",
        "\n",
        "# === CONFIG ===\n",
        "BRIGHTDATA_API_TOKEN = \"3a6ea3c3-d7b4-458e-ab9c-86c72c662b16\"\n",
        "BATCH_SIZE = 500\n",
        "MAX_WORKERS = 10\n",
        "INPUT_FILE = \"companies_with_websites_full.csv\"\n",
        "OUTPUT_DIR = \"email_batches\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# === BRIGHT DATA REQUEST ===\n",
        "def fetch_html_brightdata(url):\n",
        "    try:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {BRIGHTDATA_API_TOKEN}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        data = {\n",
        "            \"url\": url,\n",
        "            \"zone\": \"clutch\",  # or your actual zone name\n",
        "            \"format\": \"raw\",\n",
        "        }\n",
        "        response = requests.post(\n",
        "            \"https://api.brightdata.com/request\",\n",
        "            json=data,\n",
        "            headers=headers,\n",
        "            timeout=10\n",
        "        )\n",
        "        if response.ok:\n",
        "            return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error fetching {url}: {e}\")\n",
        "    return \"\"\n",
        "\n",
        "# === EMAIL EXTRACTOR ===\n",
        "def extract_first_email(text):\n",
        "    for match in re.finditer(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.(?:com|org|net|edu)\", text):\n",
        "        email = match.group(0)\n",
        "        if not email.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "            return email\n",
        "    return \"\"\n",
        "\n",
        "# === FIND CONTACT PAGE ===\n",
        "def find_contact_url(html, base_url):\n",
        "    try:\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        base_domain = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(base_url))\n",
        "        for a_tag in soup.find_all(\"a\", href=True):\n",
        "            href = a_tag['href'].lower()\n",
        "            if \"contact\" in href and not any(x in href for x in [\"facebook\", \"instagram\", \"mailto:\", \".jpg\", \".pdf\"]):\n",
        "                return urljoin(base_domain, href)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# === PROCESS SINGLE COMPANY ===\n",
        "def process_row(row):\n",
        "    site = row[\"Website URL\"]\n",
        "    email = \"\"\n",
        "    try:\n",
        "        html = fetch_html_brightdata(site)\n",
        "        email = extract_first_email(html)\n",
        "        if not email:\n",
        "            contact_url = find_contact_url(html, site)\n",
        "            if contact_url:\n",
        "                contact_html = fetch_html_brightdata(contact_url)\n",
        "                email = extract_first_email(contact_html)\n",
        "    except:\n",
        "        pass\n",
        "    return email\n",
        "\n",
        "# === PROCESS BATCH ===\n",
        "def process_batch(batch_df, batch_num):\n",
        "    emails = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        futures = [executor.submit(process_row, row) for _, row in batch_df.iterrows()]\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                email = future.result()\n",
        "            except:\n",
        "                email = \"\"\n",
        "            emails.append(email)\n",
        "\n",
        "    # Ensure same length before writing\n",
        "    batch_df[\"email\"] = emails[:len(batch_df)]\n",
        "    batch_path = f\"{OUTPUT_DIR}/emails_batch_{batch_num}.csv\"\n",
        "    batch_df.to_csv(batch_path, index=False)\n",
        "    print(f\"✅ Saved batch {batch_num} to {batch_path}\")\n",
        "\n",
        "\n",
        "# === LOAD DATA & RUN ===\n",
        "df = pd.read_csv(INPUT_FILE).head(500)\n",
        "df = df.dropna(subset=[\"Website URL\"]).reset_index(drop=True)\n",
        "\n",
        "for start in range(0, len(df), BATCH_SIZE):\n",
        "    end = min(start + BATCH_SIZE, len(df))\n",
        "    batch_df = df.iloc[start:end].copy()\n",
        "    batch_num = start // BATCH_SIZE + 1\n",
        "    process_batch(batch_df, batch_num)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBHgmSuiNxmy",
        "outputId": "ffb4afb4-c2bc-4db7-f251-408999b893fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Processing batch 1/15...\n",
            "✅ Saved email_batches_1/batch_1.csv\n",
            "🚀 Processing batch 2/15...\n",
            "✅ Saved email_batches_1/batch_2.csv\n",
            "🚀 Processing batch 3/15...\n",
            "✅ Saved email_batches_1/batch_3.csv\n",
            "🚀 Processing batch 4/15...\n",
            "✅ Saved email_batches_1/batch_4.csv\n",
            "🚀 Processing batch 5/15...\n",
            "✅ Saved email_batches_1/batch_5.csv\n",
            "🚀 Processing batch 6/15...\n",
            "✅ Saved email_batches_1/batch_6.csv\n",
            "🚀 Processing batch 7/15...\n",
            "✅ Saved email_batches_1/batch_7.csv\n",
            "🚀 Processing batch 8/15...\n",
            "✅ Saved email_batches_1/batch_8.csv\n",
            "🚀 Processing batch 9/15...\n",
            "✅ Saved email_batches_1/batch_9.csv\n",
            "🚀 Processing batch 10/15...\n",
            "✅ Saved email_batches_1/batch_10.csv\n",
            "🚀 Processing batch 11/15...\n",
            "✅ Saved email_batches_1/batch_11.csv\n",
            "🚀 Processing batch 12/15...\n",
            "✅ Saved email_batches_1/batch_12.csv\n",
            "🚀 Processing batch 13/15...\n",
            "✅ Saved email_batches_1/batch_13.csv\n",
            "🚀 Processing batch 14/15...\n",
            "✅ Saved email_batches_1/batch_14.csv\n",
            "🚀 Processing batch 15/15...\n",
            "✅ Saved email_batches_1/batch_15.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import math\n",
        "\n",
        "# ----------------- CONFIGURATION -----------------\n",
        "BRIGHT_DATA_API_KEY = \"bc218b6f2fa1fa2a24f87485659ced353e40b344270c5de00130562eb33f4f81\"  # 🔁 Replace with your Bright Data API key\n",
        "ZONE_NAME = \"clutch\"                 # 🔁 Replace with your zone name\n",
        "CSV_FILE = \"companies_with_websites_full.csv\"\n",
        "OUTPUT_DIR = \"email_batches_1\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "BATCH_SIZE = 500\n",
        "MAX_WORKERS = 50\n",
        "# -------------------------------------------------\n",
        "\n",
        "API_URL = \"https://api.brightdata.com/request\"\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {BRIGHT_DATA_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "def extract_first_email(text):\n",
        "    for match in re.finditer(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.(?:com|org|net|edu)\", text):\n",
        "        email = match.group(0)\n",
        "        if not email.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "            return email\n",
        "    return \"\"\n",
        "\n",
        "def find_contact_url(html, base_url):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    base_domain = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(base_url))\n",
        "    contact_keywords = [\"contact\", \"contact-us\", \"contactus\"]\n",
        "    ignore_keywords = [\"facebook\", \"linkedin\", \"instagram\", \".jpg\", \".png\", \"mailto\"]\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a['href'].lower()\n",
        "        if any(k in href for k in contact_keywords) and not any(b in href for b in ignore_keywords):\n",
        "            return urljoin(base_domain, href)\n",
        "    return None\n",
        "\n",
        "def scrape_email(row):\n",
        "    idx, site = row\n",
        "    try:\n",
        "        data = {\n",
        "            \"zone\": ZONE_NAME,\n",
        "            \"url\": site,\n",
        "            \"format\": \"raw\"\n",
        "        }\n",
        "\n",
        "        # Homepage\n",
        "        res = requests.post(API_URL, headers=HEADERS, json=data, timeout=10)\n",
        "        # print(f\"Status: {res.status_code}, Length: {len(res.text)}\")\n",
        "        html = res.text\n",
        "        email = extract_first_email(html)\n",
        "        if email:\n",
        "            return idx, email\n",
        "\n",
        "        # Contact fallback\n",
        "        contact_url = find_contact_url(html, site)\n",
        "        if contact_url:\n",
        "            data[\"url\"] = contact_url\n",
        "            res2 = requests.post(API_URL, headers=HEADERS, json=data, timeout=10)\n",
        "            contact_html = res2.text\n",
        "            email = extract_first_email(contact_html)\n",
        "            return idx, email if email else \"\"\n",
        "    except:\n",
        "        pass\n",
        "    return idx, \"\"\n",
        "\n",
        "# Load and clean CSV\n",
        "df = pd.read_csv(CSV_FILE).dropna(subset=[\"Website URL\"]).reset_index(drop=True)\n",
        "df[\"email\"] = \"\"\n",
        "\n",
        "# Batch Processing\n",
        "total_batches = math.ceil(len(df) / BATCH_SIZE)\n",
        "\n",
        "for batch_num in range(total_batches):\n",
        "    print(f\"🚀 Processing batch {batch_num + 1}/{total_batches}...\")\n",
        "\n",
        "    batch_df = df.iloc[batch_num * BATCH_SIZE:(batch_num + 1) * BATCH_SIZE].copy()\n",
        "    batch_rows = list(batch_df[\"Website URL\"].items())\n",
        "\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        for result in executor.map(scrape_email, batch_rows):\n",
        "            results.append(result)\n",
        "\n",
        "    # Update emails\n",
        "    for rel_idx, email in results:\n",
        "        batch_df.at[rel_idx, \"email\"] = email\n",
        "\n",
        "    # Save batch result\n",
        "    output_file = f\"{OUTPUT_DIR}/batch_{batch_num + 1}.csv\"\n",
        "    batch_df.to_csv(output_file, index=False)\n",
        "    print(f\"✅ Saved {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHi1EznG_CfG",
        "outputId": "280e255d-ae60-43ec-d576-7d87854a0b66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Combined all emails into all_emails_combined.csv\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "all_files = glob.glob(f\"{OUTPUT_DIR}/emails_batch_*.csv\")\n",
        "combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
        "combined_df.to_csv(\"all_emails_combined.csv\", index=False)\n",
        "print(\"✅ Combined all emails into all_emails_combined.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZN6rA9oIat4",
        "outputId": "b1497f64-2129-4be7-8a70-f116f2f68b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Rows with blank emails removed and saved to filtered_emails.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your combined file\n",
        "df = pd.read_csv(\"all_emails_combined.csv\")  # Replace with your actual filename\n",
        "\n",
        "# Drop rows where 'email' column is empty or just whitespace\n",
        "df = df[~df['email'].isna() & df['email'].str.strip().astype(bool)]\n",
        "\n",
        "# Save the cleaned file\n",
        "df.to_csv(\"filtered_emails.csv\", index=False)\n",
        "\n",
        "print(\"✅ Rows with blank emails removed and saved to filtered_emails.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrapping data for limousine companies"
      ],
      "metadata": {
        "id": "chAW6ntSldTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking html lines\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "SEARCH_URL = \"https://www.google.com/maps/search/limousine+companies+in+California/@33.6162684,-118.5064845,10z/data=!3m1!4b1?entry=ttu&g_ep=EgoyMDI1MDcyMC4wIKXMDSoASAFQAw%3D%3D\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer bc218b6f2fa1fa2a24f87485659ced353e40b344270c5de00130562eb33f4f81\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "data = {\n",
        "    \"zone\": \"google_map_limo\",\n",
        "    \"url\": SEARCH_URL,\n",
        "    \"format\": \"raw\"\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    \"https://api.brightdata.com/request\",\n",
        "    json=data,\n",
        "    headers=headers,\n",
        "    timeout = 10\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    print(soup.prettify())  # view and extract names, websites, addresses, etc.\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ],
      "metadata": {
        "id": "xk5Vz84HlgxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "API_TOKEN = \"bc218b6f2fa1fa2a24f87485659ced353e40b344270c5de00130562eb33f4f81\"\n",
        "DATASET_ID = \"gd_lh0tnzlo2bie4uhdhr\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "payload = {\n",
        "        \"dataset_id\": \"gd_m8ebnr0q2qlklc02fz\",\n",
        "        \"filter\": {\n",
        "        \"operator\": \"and\",\n",
        "        \"filters\": [\n",
        "            {\"name\": \"category\", \"value\": \"limo\", \"operator\": \"includes\"},\n",
        "            {\"name\": \"country\", \"value\": \"United States\", \"operator\": \"=\"},\n",
        "            {\"name\": \"address\", \"value\": \"NJ\", \"operator\": \"not_includes\"},\n",
        "            {\"name\": \"address\", \"value\": \"NY\", \"operator\": \"not_includes\"},\n",
        "            {\"name\": \"address\", \"value\": \"CA\", \"operator\": \"not_includes\"},\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "response = requests.post(\"https://api.brightdata.com/datasets/filter\", headers=headers, json=payload)\n",
        "\n",
        "if response.ok:\n",
        "    print(\"Request succeeded:\", response.json())\n",
        "\n",
        "    # data = response.json()[\"data\"]  # Assuming the filtered result is under \"data\"\n",
        "    # df = pd.json_normalize(data)\n",
        "    # df.to_csv(\"limousine_CA.csv\", index=False)\n",
        "    # print(\"✅ Data saved to limousine_CA.csv\")\n",
        "else:\n",
        "    print(\"❌ Request failed:\", response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yO9eC5jm26c",
        "outputId": "2a9a587f-e1ed-478e-ca96-20d1f2fb5e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request succeeded: {'snapshot_id': 'snap_mebzvtie2t2s1zeev'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing json lines\n",
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "API_TOKEN = \"bc218b6f2fa1fa2a24f87485659ced353e40b344270c5de00130562eb33f4f81\"\n",
        "SNAPSHOT_ID = \"snap_mebzvtie2t2s1zeev\"\n",
        "\n",
        "url = f\"https://api.brightdata.com/datasets/snapshots/{SNAPSHOT_ID}/download\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "\n",
        "# Step 1: Get the download URL\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.ok:\n",
        "  data = response.text\n",
        "  print(data[:1000])\n",
        "else:\n",
        "  print(f\"❌ Failed to download: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYmShEbCywRG",
        "outputId": "c9b0fc55-bf2a-4bfd-9bd0-13fc4234d191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"address\":\"55345 US-90, Slidell, LA 70461\",\"business_details\":[{\"details\":\"C E Transportations\",\"field_name\":\"storefront\",\"link\":null},{\"details\":\"Limousine service\",\"field_name\":\"category\",\"link\":null},{\"details\":\"55345 US-90, Slidell, LA 70461\",\"field_name\":\"location\",\"link\":null},{\"details\":\"(504) 355-4074\",\"field_name\":\"call\",\"link\":null}],\"category\":\"Limousine service\",\"cid_location\":\"6480091173110706974\",\"country\":\"United States\",\"description\":null,\"directory_categories\":null,\"directory_locations\":null,\"fid_location\":\"0x889dde665878872d:0x59ede896c568731e\",\"hotel_amenities\":null,\"hotel_star_ratings\":null,\"is_claimed\":false,\"lat\":30.231533,\"lon\":-89.669867,\"main_image\":\"https://streetviewpixels-pa.googleapis.com/v1/thumbnail?panoid=byPSWGGL5M-mlGVpiU22_w&cb_client=search.gws-prod.gps&w=360&h=120&yaw=355.64862&pitch=0&thumbfov=100\",\"name\":\"C E Transportations\",\"open_hours\":null,\"open_website\":null,\"people_also_search\":[{\"category\":\"Limousine service\",\"name\":\"VIP/CELEBRITY LIMOUSIN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "API_TOKEN = \"bc218b6f2fa1fa2a24f87485659ced353e40b344270c5de00130562eb33f4f81\"\n",
        "SNAPSHOT_ID = \"snap_mebzvtie2t2s1zeev\"\n",
        "\n",
        "url = f\"https://api.brightdata.com/datasets/snapshots/{SNAPSHOT_ID}/download\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "\n",
        "# Step 1: Get the download URL\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.ok:\n",
        "    lines = response.text.strip().split(\"\\n\")\n",
        "    records = [json.loads(line) for line in lines]\n",
        "\n",
        "    # === EXTRACT RELEVANT FIELDS ===\n",
        "    extracted = []\n",
        "    for item in records:\n",
        "        extracted.append({\n",
        "            \"name\": item.get(\"name\"),\n",
        "            \"address\": item.get(\"address\"),\n",
        "            \"phone\": item.get(\"phone_number\"),\n",
        "            \"website\": item.get(\"open_website\"),\n",
        "            \"google_maps_url\": item.get(\"url\"),\n",
        "            \"latitude\": item.get(\"lat\"),\n",
        "            \"longitude\": item.get(\"lon\")\n",
        "        })\n",
        "\n",
        "    # === SAVE TO CSV ===\n",
        "    df = pd.DataFrame(extracted)\n",
        "    df.to_csv(\"limousine_brightdata.csv\", index=False)\n",
        "    print(\"✅ Data saved to 'limousine_brightdata.csv'\")\n",
        "else:\n",
        "    print(\"❌ Failed to fetch snapshot:\", response.status_code, response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6ZghLBlwSbu",
        "outputId": "a86103d2-029e-4942-f1c9-c71445763a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data saved to 'limousine_brightdata.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV\n",
        "df = pd.read_csv(\"limousine_brightdata.csv\")\n",
        "\n",
        "# Drop rows where 'website' is NaN or does not start with 'http'\n",
        "df_cleaned = df[df['website'].notna() & df['website'].str.lower().str.startswith('http')]\n",
        "\n",
        "# Save the cleaned CSV\n",
        "df_cleaned.to_csv(\"limousine_brightdata_cleaned.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Cleaned CSV saved as 'limousine_brightdata_cleaned.csv' with {len(df_cleaned)} rows.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHpk4KGu1E0a",
        "outputId": "28d0738a-052e-4773-817b-f02c04f007a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned CSV saved as 'limousine_brightdata_cleaned.csv' with 1942 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New Version with ChatGPT 5.0"
      ],
      "metadata": {
        "id": "boLKFxiJ0Flz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting and Configuration\n",
        "\n",
        "# --- Install deps (usually present in Colab, but safe) ---\n",
        "!pip -q install beautifulsoup4 tqdm\n",
        "\n",
        "import os, math, time, io, json, re, html\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from getpass import getpass\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "CSV_FILE = \"/content/limousine_brightdata_cleaned.csv\"  # input with a 'website' column\n",
        "OUTPUT_DIR = \"email_batches\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE   = 100          # rows per batch file\n",
        "MAX_WORKERS  = 20           # parallel requests (mind your Bright Data limits)\n",
        "ZONE_NAME    = \"clutch\"     # your Bright Data zone name\n",
        "REQUEST_TIMEOUT = 25        # seconds per page fetch\n",
        "RETRIES = 2                 # transient retry attempts\n",
        "BACKOFF_SECS = 2.0          # linear backoff base\n",
        "# ------------------------------------------\n",
        "\n",
        "# API key (do NOT hardcode); rotate if previously shared\n",
        "BRIGHT_DATA_API_KEY = getpass(\"Paste your Bright Data API key (hidden): \").strip()\n",
        "if not BRIGHT_DATA_API_KEY:\n",
        "    raise ValueError(\"No API key provided.\")\n",
        "\n",
        "API_URL = \"https://api.brightdata.com/request\"\n",
        "SESSION_HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {BRIGHT_DATA_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peTIrYsw0Bnf",
        "outputId": "d052b35d-5c10-43f9-d544-b605c5259303"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your Bright Data API key (hidden): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Email extraction helpers\n",
        "\n",
        "EMAIL_RE = re.compile(r\"[a-zA-Z0-9._%+\\-]+@[a-zA-Z0-9.\\-]+\\.[A-Za-z]{2,}\")\n",
        "\n",
        "# Common obfuscations -> normalize before regex\n",
        "OBFUSCATIONS = [\n",
        "    (r\"\\s*\\[\\s*at\\s*\\]\\s*\", \"@\"),\n",
        "    (r\"\\s*\\(\\s*at\\s*\\)\\s*\", \"@\"),\n",
        "    (r\"\\s+at\\s+\", \"@\"),\n",
        "    (r\"\\s*\\{\\s*at\\s*\\}\\s*\", \"@\"),\n",
        "    (r\"\\s*\\[\\s*dot\\s*\\]\\s*\", \".\"),\n",
        "    (r\"\\s*\\(\\s*dot\\s*\\)\\s*\", \".\"),\n",
        "    (r\"\\s+dot\\s+\", \".\"),\n",
        "    (r\"\\s*\\{\\s*dot\\s*\\}\\s*\", \".\"),\n",
        "]\n",
        "\n",
        "def _norm_text_for_email(text: str) -> str:\n",
        "    t = html.unescape(text or \"\")\n",
        "    for pat, repl in OBFUSCATIONS:\n",
        "        t = re.sub(pat, repl, t, flags=re.IGNORECASE)\n",
        "    return t\n",
        "\n",
        "def _strip_mailto_params(addr: str) -> str:\n",
        "    if addr.lower().startswith(\"mailto:\"):\n",
        "        addr = addr[len(\"mailto:\"):]\n",
        "    return addr.split(\"?\", 1)[0].strip()\n",
        "\n",
        "def _decode_cloudflare_cfemail(hex_str: str) -> str:\n",
        "    try:\n",
        "        key = int(hex_str[:2], 16)\n",
        "        return ''.join(chr(int(hex_str[i:i+2], 16) ^ key) for i in range(2, len(hex_str), 2))\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def extract_emails_from_html(html_text: str):\n",
        "    \"\"\"\n",
        "    Returns (emails_set, discovery_details_list)\n",
        "    discovery_details_list: list of (method, value) to help debug how we found it\n",
        "    \"\"\"\n",
        "    emails, details = set(), []\n",
        "    if not html_text:\n",
        "        return emails, details\n",
        "\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "\n",
        "    # 1) mailto: links\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        if href.lower().startswith(\"mailto:\"):\n",
        "            email = _strip_mailto_params(href)\n",
        "            if EMAIL_RE.fullmatch(email):\n",
        "                emails.add(email)\n",
        "                details.append((\"mailto\", email))\n",
        "\n",
        "    # 2) Cloudflare-protected emails\n",
        "    for el in soup.select(\"[data-cfemail]\"):\n",
        "        hex_str = el.get(\"data-cfemail\", \"\").strip()\n",
        "        decoded = _decode_cloudflare_cfemail(hex_str)\n",
        "        if EMAIL_RE.fullmatch(decoded):\n",
        "            emails.add(decoded)\n",
        "            details.append((\"cfemail\", decoded))\n",
        "\n",
        "    # 3) JSON-LD blocks that contain \"email\": \"...\"\n",
        "    for script in soup.find_all(\"script\", type=lambda t: t and \"ld+json\" in t):\n",
        "        try:\n",
        "            data = json.loads(script.string or \"\")\n",
        "            objs = data if isinstance(data, list) else [data]\n",
        "            for obj in objs:\n",
        "                if isinstance(obj, dict) and \"email\" in obj:\n",
        "                    val = obj[\"email\"]\n",
        "                    if isinstance(val, str):\n",
        "                        val = _strip_mailto_params(val)\n",
        "                        if EMAIL_RE.fullmatch(val):\n",
        "                            emails.add(val)\n",
        "                            details.append((\"jsonld\", val))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 4) Visible text (with obfuscation normalization)\n",
        "    text = _norm_text_for_email(soup.get_text(separator=\" \"))\n",
        "    for m in EMAIL_RE.finditer(text):\n",
        "        email = m.group(0)\n",
        "        emails.add(email)\n",
        "        details.append((\"text\", email))\n",
        "\n",
        "    return emails, details\n"
      ],
      "metadata": {
        "id": "CUnRk-kD0bNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Page Discovery + BrightData fetch with retries\n",
        "\n",
        "CONTACT_KEYWORDS = [\n",
        "    \"contact\", \"contact-us\", \"contactus\", \"contact_us\",\n",
        "    \"about\", \"about-us\", \"aboutus\",\n",
        "    \"support\", \"help\", \"customer-service\",\n",
        "    \"team\", \"company\", \"impressum\", \"kontakt\",\n",
        "    \"privacy\", \"terms\"\n",
        "]\n",
        "\n",
        "def same_domain(url_a, url_b):\n",
        "    return urlparse(url_a).netloc == urlparse(url_b).netloc\n",
        "\n",
        "def find_candidate_pages(html_text: str, base_url: str, limit: int = 6) -> list:\n",
        "    soup = BeautifulSoup(html_text or \"\", \"html.parser\")\n",
        "    base = \"{u.scheme}://{u.netloc}\".format(u=urlparse(base_url))\n",
        "    cand, seen = [], set()\n",
        "\n",
        "    def add(url):\n",
        "        if not url: return\n",
        "        if not urlparse(url).scheme:\n",
        "            url = urljoin(base, url)\n",
        "        if not same_domain(url, base):\n",
        "            return\n",
        "        if url not in seen:\n",
        "            seen.add(url)\n",
        "            cand.append(url)\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        low = href.lower()\n",
        "        if any(k in low for k in CONTACT_KEYWORDS):\n",
        "            add(href)\n",
        "\n",
        "    for sel in [\"footer a[href]\", \"nav a[href]\"]:\n",
        "        for a in soup.select(sel):\n",
        "            href = a.get(\"href\", \"\").strip()\n",
        "            low = href.lower()\n",
        "            if any(k in low for k in CONTACT_KEYWORDS):\n",
        "                add(href)\n",
        "\n",
        "    # Include /contact as a strong guess\n",
        "    add(urljoin(base, \"/contact\"))\n",
        "\n",
        "    return cand[:limit]\n",
        "\n",
        "def fetch_html_via_brightdata(url: str, zone: str, timeout=25, retries=2, backoff=2.0) -> str:\n",
        "    payload = {\"zone\": zone, \"url\": url, \"format\": \"raw\"}\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            r = requests.post(API_URL, headers=SESSION_HEADERS, json=payload, timeout=timeout)\n",
        "            if r.status_code == 200 and r.text:\n",
        "                return r.text\n",
        "            if r.status_code >= 500 or r.status_code in (408, 429, 202):\n",
        "                raise RuntimeError(f\"Transient code {r.status_code}\")\n",
        "            return \"\"  # non-retryable (e.g., 4xx)\n",
        "        except Exception:\n",
        "            if attempt < retries:\n",
        "                time.sleep(backoff * (attempt + 1))\n",
        "            else:\n",
        "                return \"\"\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "CpdAIrC80_jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The scraper for one site (returns email + where/how it was found)\n",
        "\n",
        "def rank_choice(emails: set) -> str:\n",
        "    # Prefer non-generic, then shorter (heuristic)\n",
        "    def key_fn(e):\n",
        "        generic = e.startswith((\"info@\", \"admin@\", \"noreply@\", \"no-reply@\", \"office@\", \"hello@\"))\n",
        "        return (generic, len(e))\n",
        "    return sorted(emails, key=key_fn)[0]\n",
        "\n",
        "def scrape_email_for_site(idx: int, site: str):\n",
        "    \"\"\"\n",
        "    Returns dict with:\n",
        "      index, website, email, source_url, method, attempts, notes\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        \"index\": idx,\n",
        "        \"website\": site,\n",
        "        \"email\": \"\",\n",
        "        \"source_url\": \"\",\n",
        "        \"method\": \"\",\n",
        "        \"attempts\": 0,\n",
        "        \"notes\": \"\"\n",
        "    }\n",
        "    try:\n",
        "        # 1) Homepage\n",
        "        home_html = fetch_html_via_brightdata(site, ZONE_NAME, timeout=REQUEST_TIMEOUT,\n",
        "                                              retries=RETRIES, backoff=BACKOFF_SECS)\n",
        "        result[\"attempts\"] += 1\n",
        "        emails, details = extract_emails_from_html(home_html)\n",
        "        if emails:\n",
        "            result[\"email\"] = rank_choice(emails)\n",
        "            # pick the most \"direct\" method seen if present\n",
        "            method_priority = {\"mailto\": 0, \"cfemail\": 1, \"jsonld\": 2, \"text\": 3}\n",
        "            method = sorted(details, key=lambda d: method_priority.get(d[0], 99))[0][0]\n",
        "            result[\"method\"] = method\n",
        "            result[\"source_url\"] = site\n",
        "            return result\n",
        "\n",
        "        # 2) Candidate pages\n",
        "        candidates = find_candidate_pages(home_html, site, limit=6)\n",
        "        for url in candidates:\n",
        "            html_page = fetch_html_via_brightdata(url, ZONE_NAME, timeout=REQUEST_TIMEOUT,\n",
        "                                                  retries=RETRIES, backoff=BACKOFF_SECS)\n",
        "            result[\"attempts\"] += 1\n",
        "            emails, details = extract_emails_from_html(html_page)\n",
        "            if emails:\n",
        "                result[\"email\"] = rank_choice(emails)\n",
        "                method_priority = {\"mailto\": 0, \"cfemail\": 1, \"jsonld\": 2, \"text\": 3}\n",
        "                method = sorted(details, key=lambda d: method_priority.get(d[0], 99))[0][0]\n",
        "                result[\"method\"] = method\n",
        "                result[\"source_url\"] = url\n",
        "                return result\n",
        "\n",
        "        # 3) Last-resort common slugs\n",
        "        for slug in [\"/contact\", \"/about\", \"/support\", \"/privacy\", \"/terms\"]:\n",
        "            url = urljoin(site, slug)\n",
        "            html_page = fetch_html_via_brightdata(url, ZONE_NAME, timeout=REQUEST_TIMEOUT,\n",
        "                                                  retries=1, backoff=BACKOFF_SECS)\n",
        "            result[\"attempts\"] += 1\n",
        "            if not html_page:\n",
        "                continue\n",
        "            emails, details = extract_emails_from_html(html_page)\n",
        "            if emails:\n",
        "                result[\"email\"] = rank_choice(emails)\n",
        "                method_priority = {\"mailto\": 0, \"cfemail\": 1, \"jsonld\": 2, \"text\": 3}\n",
        "                method = sorted(details, key=lambda d: method_priority.get(d[0], 99))[0][0]\n",
        "                result[\"method\"] = method\n",
        "                result[\"source_url\"] = url\n",
        "                return result\n",
        "\n",
        "        result[\"notes\"] = \"No email found (contact form only or obfuscated beyond heuristics)\"\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        result[\"notes\"] = f\"Error: {e}\"\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "nar86Hq71Ffo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load input, run in batches with concurrency, and save outputs\n",
        "\n",
        "# Load & pre-clean CSV\n",
        "df = pd.read_csv(CSV_FILE)\n",
        "if \"website\" not in df.columns:\n",
        "    raise ValueError(\"Input CSV must have a 'website' column.\")\n",
        "\n",
        "df = df.dropna(subset=[\"website\"]).reset_index(drop=True)\n",
        "df = df[df[\"website\"].str.lower().str.startswith((\"http://\", \"https://\"))].copy()\n",
        "df[\"website\"] = df[\"website\"].str.strip()\n",
        "\n",
        "total_batches = math.ceil(len(df) / BATCH_SIZE)\n",
        "print(f\"Total rows: {len(df)} | Batches: {total_batches}\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for batch_num in range(total_batches):\n",
        "    start = batch_num * BATCH_SIZE\n",
        "    end   = min((batch_num + 1) * BATCH_SIZE, len(df))\n",
        "    batch_df = df.iloc[start:end].copy()\n",
        "\n",
        "    print(f\"\\n🚀 Processing batch {batch_num + 1}/{total_batches} (rows {start}–{end-1})\")\n",
        "    futures = []\n",
        "    results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        for idx, site in batch_df[\"website\"].items():\n",
        "            futures.append(ex.submit(scrape_email_for_site, idx, site))\n",
        "\n",
        "        for f in tqdm(as_completed(futures), total=len(futures), leave=False):\n",
        "            results.append(f.result())\n",
        "\n",
        "    # Merge results back to batch_df\n",
        "    res_df = pd.DataFrame(results).set_index(\"index\")\n",
        "    for col in [\"email\", \"source_url\", \"method\", \"attempts\", \"notes\"]:\n",
        "        batch_df[col] = res_df[col]\n",
        "\n",
        "    # Save batch output\n",
        "    out_path = f\"{OUTPUT_DIR}/batch_{batch_num + 1}.csv\"\n",
        "    batch_df.to_csv(out_path, index=False)\n",
        "    print(f\"✅ Saved {out_path}  | found {batch_df['email'].notna().sum()} (non-empty: {(batch_df['email']!='').sum()})\")\n",
        "\n",
        "    all_results.append(batch_df)\n",
        "\n",
        "# Save combined\n",
        "combined = pd.concat(all_results, ignore_index=True)\n",
        "combined_out = \"/content/emails_combined.csv\"\n",
        "combined.to_csv(combined_out, index=False)\n",
        "print(f\"\\n🎉 Done. Combined CSV at: {combined_out}\")\n"
      ],
      "metadata": {
        "id": "u-Qrzxqg1Ll4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faster Extraction with GPT 5.0"
      ],
      "metadata": {
        "id": "Q3S1GirY3JHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Prep (Removing unwanted columns)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# One-liner approach\n",
        "df = pd.read_csv('/content/comprehensive limo companies in alberta canada - alberta_major_cities.csv')\n",
        "df[[\"google_place_url\", \"business_name\", \"business_website\", \"business_phone\", \"intl_phone\", \"full_address\"]].to_csv('filtered_output.csv', index=False)\n",
        "print(\"CSV filtered successfully!\")"
      ],
      "metadata": {
        "id": "Mza43zCHz-WF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a8268d-3209-418c-d6a5-7252633573c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV filtered successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting email from the websites (FAST)\n",
        "\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import threading\n",
        "\n",
        "# ----------------- CONFIGURATION -----------------\n",
        "BRIGHT_DATA_API_KEY = \"bc218b6f2fa1fa2a24f87485659ced353e40b344270c5de00130562eb33f4f81\"  # ← rotate yours; don't commit\n",
        "ZONE_NAME = \"clutch\"\n",
        "CSV_FILE = \"/content/filtered_output.csv\"\n",
        "OUTPUT_DIR = \"email_batches_1\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "BATCH_SIZE = 100\n",
        "MAX_WORKERS = 20\n",
        "\n",
        "API_URL = \"https://api.brightdata.com/request\"\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {BRIGHT_DATA_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# ---- Speed helpers ----\n",
        "# Precompile regex once\n",
        "EMAIL_RE = re.compile(r\"[a-zA-Z0-9._%+\\-]+@[a-zA-Z0-9.\\-]+\\.[A-Za-z]{2,}\", re.I)\n",
        "\n",
        "# Thread-local session for keep-alive + connection pooling\n",
        "_thread_local = threading.local()\n",
        "def get_session():\n",
        "    s = getattr(_thread_local, \"session\", None)\n",
        "    if s is None:\n",
        "        s = requests.Session()\n",
        "        s.headers.update(HEADERS)\n",
        "        _thread_local.session = s\n",
        "    return s\n",
        "\n",
        "# Extract the first email quickly from raw HTML (includes mailto: and visible text)\n",
        "def fast_email_from_raw(html_text: str) -> str:\n",
        "    if not html_text:\n",
        "        return \"\"\n",
        "    # Prioritize mailto: first\n",
        "    m = re.search(r\"mailto:([a-zA-Z0-9._%+\\-]+@[a-zA-Z0-9.\\-]+\\.[A-Za-z]{2,})\", html_text, re.I)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "    # Fallback to any email-looking string\n",
        "    m = EMAIL_RE.search(html_text)\n",
        "    return m.group(0) if m else \"\"\n",
        "\n",
        "# If we must parse, find a single \"contact\" link quickly\n",
        "def find_contact_url(html, base_url):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    base_domain = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(base_url))\n",
        "    contact_keywords = (\"contact\", \"contact-us\", \"contactus\", \"contact_us\")\n",
        "    ignore = (\"facebook\", \"linkedin\", \"instagram\", \".jpg\", \".png\", \"mailto\")\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a['href'].lower()\n",
        "        if any(k in href for k in contact_keywords) and not any(b in href for b in ignore):\n",
        "            return urljoin(base_domain, href)\n",
        "    return None\n",
        "\n",
        "def fetch_raw(url: str, timeout=8, max_bytes=1_000_000) -> str:\n",
        "    \"\"\"Fetch with Bright Data via POST; limit bytes processed for speed.\"\"\"\n",
        "    sess = get_session()\n",
        "    payload = {\"zone\": ZONE_NAME, \"url\": url, \"format\": \"raw\"}\n",
        "    try:\n",
        "        r = sess.post(API_URL, json=payload, timeout=timeout)\n",
        "        if r.status_code == 200 and r.text:\n",
        "            # Slice to first MB to avoid heavy parsing on huge pages\n",
        "            return r.text[:max_bytes]\n",
        "    except requests.RequestException:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "def scrape_email(row):\n",
        "    idx, site = row\n",
        "    try:\n",
        "        # 1) Homepage quick path: raw regex only\n",
        "        html = fetch_raw(site, timeout=8)\n",
        "        email = fast_email_from_raw(html)\n",
        "        if email:\n",
        "            return idx, email\n",
        "\n",
        "        # 2) Try a detected \"contact\" link (parse once)\n",
        "        contact_url = find_contact_url(html, site) if html else None\n",
        "        if contact_url:\n",
        "            html2 = fetch_raw(contact_url, timeout=8)\n",
        "            email = fast_email_from_raw(html2)\n",
        "            if email:\n",
        "                return idx, email\n",
        "\n",
        "        # 3) Two fixed fallbacks (direct hits) — cheap & fast\n",
        "        for slug in (\"/contact\", \"/about\"):\n",
        "            fallback = urljoin(site, slug)\n",
        "            html3 = fetch_raw(fallback, timeout=6)\n",
        "            email = fast_email_from_raw(html3)\n",
        "            if email:\n",
        "                return idx, email\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "    return idx, \"\"\n",
        "\n",
        "# ----------------- DRIVER -----------------\n",
        "df = pd.read_csv(CSV_FILE).dropna(subset=[\"business_website\"]).reset_index(drop=True)\n",
        "df = df[df['business_website'].str.lower().str.startswith(\"http\")]  # Filter invalid URLs\n",
        "df[\"email\"] = \"\"\n",
        "\n",
        "total_batches = math.ceil(len(df) / BATCH_SIZE)\n",
        "\n",
        "for batch_num in range(total_batches):\n",
        "    print(f\"🚀 Processing batch {batch_num + 1}/{total_batches}...\")\n",
        "\n",
        "    batch_df = df.iloc[batch_num * BATCH_SIZE:(batch_num + 1) * BATCH_SIZE].copy()\n",
        "    batch_rows = list(batch_df[\"business_website\"].items())\n",
        "\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        for result in executor.map(scrape_email, batch_rows):\n",
        "            results.append(result)\n",
        "\n",
        "    # Update emails\n",
        "    for rel_idx, email in results:\n",
        "        if isinstance(email, str):\n",
        "            batch_df.at[rel_idx, \"email\"] = email\n",
        "\n",
        "    # Save batch result\n",
        "    output_file = f\"{OUTPUT_DIR}/batch_{batch_num + 1}.csv\"\n",
        "    batch_df.to_csv(output_file, index=False)\n",
        "    print(f\"✅ Saved {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBy-sIJN3HXW",
        "outputId": "6f22aebc-c50d-44f0-ed1e-00b6ccd0cc7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Processing batch 1/9...\n",
            "✅ Saved email_batches_1/batch_1.csv\n",
            "🚀 Processing batch 2/9...\n",
            "✅ Saved email_batches_1/batch_2.csv\n",
            "🚀 Processing batch 3/9...\n",
            "✅ Saved email_batches_1/batch_3.csv\n",
            "🚀 Processing batch 4/9...\n",
            "✅ Saved email_batches_1/batch_4.csv\n",
            "🚀 Processing batch 5/9...\n",
            "✅ Saved email_batches_1/batch_5.csv\n",
            "🚀 Processing batch 6/9...\n",
            "✅ Saved email_batches_1/batch_6.csv\n",
            "🚀 Processing batch 7/9...\n",
            "✅ Saved email_batches_1/batch_7.csv\n",
            "🚀 Processing batch 8/9...\n",
            "✅ Saved email_batches_1/batch_8.csv\n",
            "🚀 Processing batch 9/9...\n",
            "✅ Saved email_batches_1/batch_9.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "OUTPUT_DIR = \"email_batches_1\"\n",
        "all_files = glob.glob(f\"{OUTPUT_DIR}/batch_*.csv\")\n",
        "combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
        "combined_df.to_csv(\"all_emails_combined.csv\", index=False)\n",
        "print(\"✅ Combined all emails into all_emails_combined.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11niOpCf48uV",
        "outputId": "d7a9f9b9-40d0-4a0e-dff1-51bab8ea5c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Combined all emails into all_emails_combined.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter empty email column\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV\n",
        "df = pd.read_csv(\"/content/all_emails_combined.csv\")\n",
        "\n",
        "# Keep only rows where 'email' is null/empty\n",
        "no_email_df = df[df['email'].isna() | (df['email'].str.strip() == '')]\n",
        "\n",
        "# Save to a new CSV\n",
        "no_email_df.to_csv(\"no_emails.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Saved {len(no_email_df)} rows without emails to no_emails.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ-pY0ch9OzQ",
        "outputId": "39ea52b7-b2a8-4644-cfb3-87b8ae01da71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 359 rows without emails to no_emails.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Fast Email Extractor via Bright Data (handles mailto:%20...) ===\n",
        "\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "import html\n",
        "import threading\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse, unquote\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from getpass import getpass\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "CSV_FILE        = \"/content/no_emails.csv\"  # must have 'website' column\n",
        "OUTPUT_DIR      = \"no_email_batches\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE      = 100\n",
        "MAX_WORKERS     = 20\n",
        "REQUEST_TIMEOUT = 8\n",
        "RETRIES         = 1\n",
        "ZONE_NAME       = \"clutch\"\n",
        "MAX_HTML_BYTES  = 1_000_000\n",
        "# ------------------------------------------\n",
        "\n",
        "# API key (env or prompt). Rotate if you pasted it anywhere public.\n",
        "BRIGHT_DATA_API_KEY = os.getenv(\"BRIGHTDATA_API_KEY\") or getpass(\"Paste your Bright Data API key (hidden): \").strip()\n",
        "if not BRIGHT_DATA_API_KEY:\n",
        "    raise ValueError(\"No Bright Data API key provided.\")\n",
        "\n",
        "API_URL = \"https://api.brightdata.com/request\"\n",
        "COMMON_HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {BRIGHT_DATA_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# ----------------- Regex & helpers -----------------\n",
        "EMAIL_RE = re.compile(r\"[a-zA-Z0-9._%+\\-]+@[a-zA-Z0-9.\\-]+\\.[A-Za-z]{2,}\", re.I)\n",
        "\n",
        "# Robust mailto attribute capture: href=\"mailto:.....\"\n",
        "MAILTO_ATTR_RE = re.compile(\n",
        "    r'href\\s*=\\s*[\"\\']\\s*(mailto:[^\"\\']+)[\"\\']',\n",
        "    re.I\n",
        ")\n",
        "\n",
        "def _clean_email(token: str) -> str:\n",
        "    if not token:\n",
        "        return \"\"\n",
        "    t = html.unescape(token)\n",
        "    # normalize odd spaces (NBSP/ZW spaces)\n",
        "    t = t.replace(\"\\u00a0\", \" \").replace(\"\\u200b\", \"\").replace(\"\\u200c\", \"\").replace(\"\\u200d\", \"\")\n",
        "    t = t.strip().strip(\".,;:!?)(\").strip()\n",
        "    if t.startswith(\"<\") and t.endswith(\">\"):\n",
        "        t = t[1:-1].strip()\n",
        "    return t\n",
        "\n",
        "def _split_addresses(s: str):\n",
        "    return [p.strip() for p in re.split(r\"[;,]\", s) if p.strip()]\n",
        "\n",
        "def _iterative_unquote(s: str, max_rounds: int = 3) -> str:\n",
        "    # In case content is double-encoded\n",
        "    for _ in range(max_rounds):\n",
        "        new = unquote(s)\n",
        "        if new == s:\n",
        "            break\n",
        "        s = new\n",
        "    return s\n",
        "\n",
        "def _decode_mailto_value(raw: str) -> str:\n",
        "    \"\"\"\n",
        "    Decode a mailto attribute value robustly:\n",
        "      - HTML-unescape\n",
        "      - iterative URL-decode (% encodings)\n",
        "      - remove 'mailto:' prefix (allowing whitespace after colon)\n",
        "      - strip query (?subject=..)\n",
        "    \"\"\"\n",
        "    s = html.unescape(raw or \"\")\n",
        "    s = _iterative_unquote(s)\n",
        "    # remove mailto: (allow spaces after colon)\n",
        "    s = re.sub(r'^\\s*mailto:\\s*', '', s, flags=re.I)\n",
        "    # cut off query params\n",
        "    s = s.split(\"?\", 1)[0]\n",
        "    # normalize weird spaces again\n",
        "    s = s.replace(\"\\u00a0\", \" \").strip()\n",
        "    return s\n",
        "\n",
        "def _rank_choice(emails: set) -> str:\n",
        "    def key_fn(e):\n",
        "        generic = e.lower().startswith((\"info@\", \"admin@\", \"noreply@\", \"no-reply@\", \"office@\", \"hello@\"))\n",
        "        return (generic, len(e))\n",
        "    return sorted(emails, key=key_fn)[0]\n",
        "\n",
        "# ----------------- Session pooling (per-thread) -----------------\n",
        "_thread_local = threading.local()\n",
        "def get_session():\n",
        "    s = getattr(_thread_local, \"session\", None)\n",
        "    if s is None:\n",
        "        s = requests.Session()\n",
        "        s.headers.update(COMMON_HEADERS)\n",
        "        _thread_local.session = s\n",
        "    return s\n",
        "\n",
        "def fetch_raw(url: str, timeout=REQUEST_TIMEOUT, retries=RETRIES, max_bytes=MAX_HTML_BYTES) -> str:\n",
        "    payload = {\"zone\": ZONE_NAME, \"url\": url, \"format\": \"raw\"}\n",
        "    sess = get_session()\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            r = sess.post(API_URL, json=payload, timeout=timeout)\n",
        "            if r.status_code == 200 and r.text:\n",
        "                return r.text[:max_bytes]\n",
        "            if r.status_code >= 500 or r.status_code in (408, 429, 202):\n",
        "                continue\n",
        "            return \"\"\n",
        "        except requests.RequestException:\n",
        "            if attempt < retries:\n",
        "                continue\n",
        "            return \"\"\n",
        "    return \"\"\n",
        "\n",
        "# ----------------- Email extraction -----------------\n",
        "def extract_email_pref_mailto_regex(html_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Fast and robust:\n",
        "      1) Find mailto hrefs via regex (handles mailto:%20..., mixed case, quotes)\n",
        "      2) Decode/clean; split multiple addresses; return first valid email\n",
        "      3) Fallback: any visible email in HTML text\n",
        "    \"\"\"\n",
        "    if not html_text:\n",
        "        return \"\"\n",
        "\n",
        "    # 1) mailto via regex (no DOM needed)\n",
        "    for m in MAILTO_ATTR_RE.finditer(html_text):\n",
        "        raw_mailto = m.group(1)  # e.g., 'mailto:%20cmtdenver@gmail.com?subject=Hi'\n",
        "        decoded_field = _decode_mailto_value(raw_mailto)  # -> 'cmtdenver@gmail.com' (handles %20)\n",
        "        for token in _split_addresses(decoded_field):\n",
        "            mm = EMAIL_RE.search(token)\n",
        "            if mm:\n",
        "                em = _clean_email(mm.group(0))\n",
        "                if em and not em.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
        "                    return em\n",
        "\n",
        "    # 2) Fallback: scan whole HTML for visible email\n",
        "    m = EMAIL_RE.search(html_text)\n",
        "    if m:\n",
        "        em = _clean_email(m.group(0))\n",
        "        if em and not em.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
        "            return em\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def find_contact_url_light(html_text: str, base_url: str):\n",
        "    \"\"\"Light parse to find a likely contact link; avoids heavy crawling.\"\"\"\n",
        "    if not html_text:\n",
        "        return None\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    base_domain = \"{u.scheme}://{u.netloc}\".format(u=urlparse(base_url))\n",
        "    contact_keywords = (\"contact\", \"contact-us\", \"contactus\", \"contact_us\", \"contact us\")\n",
        "    ignore = (\"facebook\", \"linkedin\", \"instagram\", \".jpg\", \".png\", \"mailto\")\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href_raw = a[\"href\"]\n",
        "        href = href_raw.lower()\n",
        "        if any(k in href for k in contact_keywords) and not any(b in href for b in ignore):\n",
        "            return urljoin(base_domain, href_raw)\n",
        "    return None\n",
        "\n",
        "# ----------------- Scraper for one site -----------------\n",
        "def scrape_email(row):\n",
        "    idx, site = row\n",
        "    try:\n",
        "        # 1) Homepage\n",
        "        html_text = fetch_raw(site)\n",
        "        email = extract_email_pref_mailto_regex(html_text)\n",
        "        if email:\n",
        "            return idx, email\n",
        "\n",
        "        # 2) Try a discovered contact link\n",
        "        contact_url = find_contact_url_light(html_text, site)\n",
        "        if contact_url:\n",
        "            html2 = fetch_raw(contact_url)\n",
        "            email = extract_email_pref_mailto_regex(html2)\n",
        "            if email:\n",
        "                return idx, email\n",
        "\n",
        "        # 3) Cheap fixed fallbacks\n",
        "        for slug in (\"/contact\", \"/about\"):\n",
        "            fb = urljoin(site, slug)\n",
        "            html3 = fetch_raw(fb, timeout=6, retries=0)\n",
        "            email = extract_email_pref_mailto_regex(html3)\n",
        "            if email:\n",
        "                return idx, email\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "    return idx, \"\"\n",
        "\n",
        "# ----------------- Driver -----------------\n",
        "df = pd.read_csv(CSV_FILE).dropna(subset=[\"business_website\"]).reset_index(drop=True)\n",
        "df = df[df[\"business_website\"].str.lower().str.startswith((\"http://\", \"https://\"))].copy()\n",
        "df[\"business_website\"] = df[\"business_website\"].str.strip()\n",
        "df[\"email\"] = \"\"\n",
        "\n",
        "total_batches = math.ceil(len(df) / BATCH_SIZE)\n",
        "print(f\"Total rows: {len(df)} | Batches: {total_batches}\")\n",
        "\n",
        "all_batches = []\n",
        "\n",
        "for batch_num in range(total_batches):\n",
        "    print(f\"\\n🚀 Processing batch {batch_num + 1}/{total_batches}...\")\n",
        "    batch_df = df.iloc[batch_num * BATCH_SIZE : (batch_num + 1) * BATCH_SIZE].copy()\n",
        "    rows = list(batch_df[\"business_website\"].items())\n",
        "\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        for res in ex.map(scrape_email, rows):\n",
        "            results.append(res)\n",
        "\n",
        "    for rel_idx, email in results:\n",
        "        if isinstance(email, str):\n",
        "            batch_df.at[rel_idx, \"email\"] = email\n",
        "\n",
        "    out_path = f\"{OUTPUT_DIR}/batch_{batch_num + 1}.csv\"\n",
        "    batch_df.to_csv(out_path, index=False)\n",
        "    print(f\"✅ Saved {out_path} | found {(batch_df['email']!='').sum()}\")\n",
        "\n",
        "    all_batches.append(batch_df)\n",
        "\n",
        "combined = pd.concat(all_batches, ignore_index=True)\n",
        "combined_out = \"/content/emails_combined_fast.csv\"\n",
        "combined.to_csv(combined_out, index=False)\n",
        "print(f\"\\n🎉 Done. Combined CSV: {combined_out}\")\n"
      ],
      "metadata": {
        "id": "TJKoLBhZ-CB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "OUTPUT_DIR = \"no_email_batches\"\n",
        "all_files = glob.glob(f\"{OUTPUT_DIR}/batch_*.csv\")\n",
        "combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
        "combined_df.to_csv(\"filtered_all_emails_combined.csv\", index=False)\n",
        "print(\"✅ Combined all emails into filtered_all_emails_combined.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORps3AdzNN4v",
        "outputId": "12a7dd1d-ee6f-4e2d-fe7b-2c09090d3482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Combined all emails into filtered_all_emails_combined.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter empty email column\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV\n",
        "df = pd.read_csv(\"/content/filtered_all_emails_combined.csv\")\n",
        "\n",
        "# Keep only rows where 'email' is null/empty\n",
        "no_email_df = df[df['email'].isna() | (df['email'].str.strip() == '')]\n",
        "\n",
        "# Save to a new CSV\n",
        "no_email_df.to_csv(\"no_emails.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Saved {len(no_email_df)} rows without emails to no_emails.csv\")\n"
      ],
      "metadata": {
        "id": "pQ0uKz7HNRE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter not empty email column\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV\n",
        "df = pd.read_csv(\"/content/all_emails_combined.csv\")\n",
        "\n",
        "# Keep only rows where 'email' is NOT null/empty\n",
        "filtered_df = df[df['email'].notna() & (df['email'].str.strip() != '')]\n",
        "\n",
        "# Save to a new CSV\n",
        "filtered_df.to_csv(\"filtered_with_emails_1.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Saved {len(filtered_df)} rows to filtered_with_emails_2.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e4nn_rGPmgp",
        "outputId": "18d18ded-ae10-4f1f-c9e8-aa5e68c6f7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 70 rows to filtered_with_emails_2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Third time extracting emails (Charm)\n",
        "\n",
        "# === Fast Email Extractor via Bright Data (robust mailto + icon/text fallback) ===\n",
        "# - Handles: mailto:%20..., quoted/unquoted href, inline styles, anchor text, icon-neighbor text\n",
        "# - Keeps speed: connection pooling, capped bytes, minimal fallbacks\n",
        "\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "import html\n",
        "import threading\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse, unquote\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from getpass import getpass\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "CSV_FILE        = \"/content/no_emails.csv\"  # must have 'website' column\n",
        "OUTPUT_DIR      = \"email_batches_fast\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE      = 100\n",
        "MAX_WORKERS     = 20\n",
        "REQUEST_TIMEOUT = 8\n",
        "RETRIES         = 1\n",
        "ZONE_NAME       = \"clutch\"\n",
        "MAX_HTML_BYTES  = 1_000_000\n",
        "# ------------------------------------------\n",
        "\n",
        "# API key (env or prompt). Do NOT hardcode in shared code.\n",
        "BRIGHT_DATA_API_KEY = os.getenv(\"BRIGHTDATA_API_KEY\") or getpass(\"Paste your Bright Data API key (hidden): \").strip()\n",
        "if not BRIGHT_DATA_API_KEY:\n",
        "    raise ValueError(\"No Bright Data API key provided.\")\n",
        "\n",
        "API_URL = \"https://api.brightdata.com/request\"\n",
        "COMMON_HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {BRIGHT_DATA_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# ----------------- Regex & helpers -----------------\n",
        "EMAIL_RE = re.compile(r\"[a-zA-Z0-9._%+\\-]+@[a-zA-Z0-9.\\-]+\\.[A-Za-z]{2,}\", re.I)\n",
        "\n",
        "# Robust mailto href capture: supports quoted OR unquoted values and stray spaces\n",
        "MAILTO_HREF_RE = re.compile(\n",
        "    r'href\\s*=\\s*(?:'\n",
        "    r'\"[^\"]*?(mailto:[^\"]+)[^\"]*\"|'     # double-quoted\n",
        "    r\"\\'[^\\']*?(mailto:[^\\']+)[^\\']*\\'|\"# single-quoted\n",
        "    r'(mailto:[^\\s>]+)'                 # unquoted\n",
        "    r')',\n",
        "    re.I\n",
        ")\n",
        "\n",
        "# Obfuscations like \"name [at] domain [dot] com\"\n",
        "OBFUSCATIONS = [\n",
        "    (r\"\\s*\\[\\s*at\\s*\\]\\s*\", \"@\"),\n",
        "    (r\"\\s*\\(\\s*at\\s*\\)\\s*\", \"@\"),\n",
        "    (r\"\\s+at\\s+\", \"@\"),\n",
        "    (r\"\\s*\\{\\s*at\\s*\\}\\s*\", \"@\"),\n",
        "    (r\"\\s*\\[\\s*dot\\s*\\]\\s*\", \".\"),\n",
        "    (r\"\\s*\\(\\s*dot\\s*\\)\\s*\", \".\"),\n",
        "    (r\"\\s+dot\\s+\", \".\"),\n",
        "    (r\"\\s*\\{\\s*dot\\s*\\}\\s*\", \".\"),\n",
        "]\n",
        "\n",
        "def _norm_obf(text: str) -> str:\n",
        "    t = html.unescape(text or \"\")\n",
        "    for pat, repl in OBFUSCATIONS:\n",
        "        t = re.sub(pat, repl, t, flags=re.IGNORECASE)\n",
        "    return t.replace(\"\\u00a0\", \" \").replace(\"\\u200b\", \"\").replace(\"\\u200c\", \"\").replace(\"\\u200d\", \"\")\n",
        "\n",
        "def _clean_email(token: str) -> str:\n",
        "    if not token:\n",
        "        return \"\"\n",
        "    t = _norm_obf(token).strip().strip(\".,;:!?)(\")\n",
        "    if t.startswith(\"<\") and t.endswith(\">\"):\n",
        "        t = t[1:-1].strip()\n",
        "    return t\n",
        "\n",
        "def _split_addresses(s: str):\n",
        "    return [p.strip() for p in re.split(r\"[;,]\", s) if p.strip()]\n",
        "\n",
        "def _iterative_unquote(s: str, rounds: int = 3) -> str:\n",
        "    for _ in range(rounds):\n",
        "        new = unquote(s)\n",
        "        if new == s:\n",
        "            break\n",
        "        s = new\n",
        "    return s\n",
        "\n",
        "def _decode_mailto_value(raw: str) -> str:\n",
        "    \"\"\"\n",
        "    Decode a mailto attribute value robustly:\n",
        "      - HTML-unescape\n",
        "      - iterative URL-decode (% encodings)\n",
        "      - remove 'mailto:' (allowing whitespace after colon)\n",
        "      - strip query (?subject=..)\n",
        "    \"\"\"\n",
        "    s = html.unescape(raw or \"\")\n",
        "    s = _iterative_unquote(s)\n",
        "    s = re.sub(r'^\\s*mailto:\\s*', '', s, flags=re.I)   # allow spaces after colon\n",
        "    s = s.split(\"?\", 1)[0]\n",
        "    return s.strip()\n",
        "\n",
        "def _decode_cfemail(hex_str: str) -> str:\n",
        "    try:\n",
        "        key = int(hex_str[:2], 16)\n",
        "        return ''.join(chr(int(hex_str[i:i+2], 16) ^ key) for i in range(2, len(hex_str), 2))\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ----------------- Session pooling (per-thread) -----------------\n",
        "_thread_local = threading.local()\n",
        "def get_session():\n",
        "    s = getattr(_thread_local, \"session\", None)\n",
        "    if s is None:\n",
        "        s = requests.Session()\n",
        "        s.headers.update(COMMON_HEADERS)\n",
        "        _thread_local.session = s\n",
        "    return s\n",
        "\n",
        "def fetch_raw(url: str, timeout=REQUEST_TIMEOUT, retries=RETRIES, max_bytes=MAX_HTML_BYTES) -> str:\n",
        "    payload = {\"zone\": ZONE_NAME, \"url\": url, \"format\": \"raw\"}\n",
        "    sess = get_session()\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            r = sess.post(API_URL, json=payload, timeout=timeout)\n",
        "            if r.status_code == 200 and r.text:\n",
        "                return r.text[:max_bytes]\n",
        "            if r.status_code >= 500 or r.status_code in (408, 429, 202):\n",
        "                continue\n",
        "            return \"\"\n",
        "        except requests.RequestException:\n",
        "            if attempt < retries:\n",
        "                continue\n",
        "            return \"\"\n",
        "    return \"\"\n",
        "\n",
        "# ----------------- Email extraction -----------------\n",
        "def extract_email_pref_mailto_regex(html_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Fast path:\n",
        "      1) Find mailto hrefs via regex (handles quoted/unquoted, mailto:%20..., styles, etc.)\n",
        "      2) Decode/clean; split multiple addresses; return first valid email\n",
        "      3) Fallback: visible email anywhere in HTML text\n",
        "    \"\"\"\n",
        "    if not html_text:\n",
        "        return \"\"\n",
        "\n",
        "    for m in MAILTO_HREF_RE.finditer(html_text):\n",
        "        raw_1, raw_2, raw_3 = m.groups()\n",
        "        raw_mailto = raw_1 or raw_2 or raw_3\n",
        "        if not raw_mailto:\n",
        "            continue\n",
        "        decoded_field = _decode_mailto_value(raw_mailto)\n",
        "        for token in _split_addresses(decoded_field):\n",
        "            mm = EMAIL_RE.search(token)\n",
        "            if mm:\n",
        "                em = _clean_email(mm.group(0))\n",
        "                if em and not em.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
        "                    return em\n",
        "\n",
        "    # Fallback: any email-looking token in the HTML\n",
        "    m = EMAIL_RE.search(_norm_obf(html_text))\n",
        "    if m:\n",
        "        em = _clean_email(m.group(0))\n",
        "        if em and not em.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
        "            return em\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def extract_email_dom_fallback(html_text: str) -> str:\n",
        "    \"\"\"\n",
        "    One DOM pass used only when fast path fails:\n",
        "      - decode Cloudflare data-cfemail\n",
        "      - scan attributes (aria-label, title, data-email, etc.)\n",
        "      - scan anchors (href mailto + anchor text)\n",
        "      - scan text near envelope icons (fa-envelope, icon-email)\n",
        "    \"\"\"\n",
        "    if not html_text:\n",
        "        return \"\"\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "\n",
        "    # Cloudflare-protected\n",
        "    for el in soup.select(\"[data-cfemail]\"):\n",
        "        em = _decode_cfemail(el.get(\"data-cfemail\", \"\").strip())\n",
        "        if EMAIL_RE.fullmatch(em or \"\"):\n",
        "            return _clean_email(em)\n",
        "\n",
        "    # Attributes\n",
        "    ATTRS = (\"aria-label\", \"title\", \"data-email\", \"data-contact\", \"data-user\", \"data-address\")\n",
        "    for tag in soup.find_all(True):\n",
        "        for attr in ATTRS:\n",
        "            val = tag.get(attr)\n",
        "            if not val:\n",
        "                continue\n",
        "            mm = EMAIL_RE.search(_norm_obf(val))\n",
        "            if mm:\n",
        "                return _clean_email(mm.group(0))\n",
        "\n",
        "    # Anchors (href + text) — DOM backup for weird quoting\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a.get(\"href\", \"\")\n",
        "        if href:\n",
        "            # decode & strip mailto if present\n",
        "            if \"mailto:\" in href.lower():\n",
        "                decoded_field = _decode_mailto_value(href)\n",
        "                for token in _split_addresses(decoded_field):\n",
        "                    mm = EMAIL_RE.search(token)\n",
        "                    if mm:\n",
        "                        return _clean_email(mm.group(0))\n",
        "        txt = a.get_text(\" \", strip=True) or \"\"\n",
        "        mm = EMAIL_RE.search(_norm_obf(txt))\n",
        "        if mm:\n",
        "            return _clean_email(mm.group(0))\n",
        "\n",
        "    # Neighbor text around envelope icons\n",
        "    for icon in soup.select(\".fa-envelope, .icon-email, [class*='icon-email']\"):\n",
        "        container = icon\n",
        "        for _ in range(3):  # walk up a few parents\n",
        "            container = container.parent\n",
        "            if not container:\n",
        "                break\n",
        "            txt = container.get_text(\" \", strip=True)\n",
        "            if not txt:\n",
        "                continue\n",
        "            mm = EMAIL_RE.search(_norm_obf(txt))\n",
        "            if mm:\n",
        "                return _clean_email(mm.group(0))\n",
        "\n",
        "    # Whole-page text as last resort (normalized)\n",
        "    mm = EMAIL_RE.search(_norm_obf(soup.get_text(\" \", strip=True)))\n",
        "    if mm:\n",
        "        return _clean_email(mm.group(0))\n",
        "    return \"\"\n",
        "\n",
        "# ----------------- Contact link discovery (lightweight) -----------------\n",
        "def find_contact_url_light(html_text: str, base_url: str):\n",
        "    if not html_text:\n",
        "        return None\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    base_domain = \"{u.scheme}://{u.netloc}\".format(u=urlparse(base_url))\n",
        "    contact_keywords = (\"contact\", \"contact-us\", \"contactus\", \"contact_us\", \"contact us\")\n",
        "    ignore = (\"facebook\", \"linkedin\", \"instagram\", \".jpg\", \".png\", \"mailto\")\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href_raw = a[\"href\"]\n",
        "        href = href_raw.lower()\n",
        "        if any(k in href for k in contact_keywords) and not any(b in href for b in ignore):\n",
        "            return urljoin(base_domain, href_raw)\n",
        "    return None\n",
        "\n",
        "# ----------------- Scraper for one site -----------------\n",
        "def scrape_email(row):\n",
        "    idx, site = row\n",
        "    try:\n",
        "        # 1) Homepage (fast)\n",
        "        html_text = fetch_raw(site)\n",
        "        email = extract_email_pref_mailto_regex(html_text)\n",
        "        if email:\n",
        "            return idx, email\n",
        "\n",
        "        # 2) DOM fallback on homepage (handles cfemail, attributes, anchors, icons)\n",
        "        email = extract_email_dom_fallback(html_text)\n",
        "        if email:\n",
        "            return idx, email\n",
        "\n",
        "        # 3) Contact link if found\n",
        "        contact_url = find_contact_url_light(html_text, site)\n",
        "        if contact_url:\n",
        "            html2 = fetch_raw(contact_url)\n",
        "            email = extract_email_pref_mailto_regex(html2) or extract_email_dom_fallback(html2)\n",
        "            if email:\n",
        "                return idx, email\n",
        "\n",
        "        # 4) Two cheap fixed fallbacks\n",
        "        for slug in (\"/contact\", \"/about\"):\n",
        "            fb = urljoin(site, slug)\n",
        "            html3 = fetch_raw(fb, timeout=6, retries=0)\n",
        "            email = extract_email_pref_mailto_regex(html3) or extract_email_dom_fallback(html3)\n",
        "            if email:\n",
        "                return idx, email\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "    return idx, \"\"\n",
        "\n",
        "# ----------------- Driver -----------------\n",
        "df = pd.read_csv(CSV_FILE).dropna(subset=[\"business_website\"]).reset_index(drop=True)\n",
        "df = df[df[\"business_website\"].str.lower().str.startswith((\"http://\", \"https://\"))].copy()\n",
        "df[\"business_website\"] = df[\"business_website\"].str.strip()\n",
        "df[\"email\"] = \"\"\n",
        "\n",
        "total_batches = math.ceil(len(df) / BATCH_SIZE)\n",
        "print(f\"Total rows: {len(df)} | Batches: {total_batches}\")\n",
        "\n",
        "all_batches = []\n",
        "\n",
        "for batch_num in range(total_batches):\n",
        "    print(f\"\\n🚀 Processing batch {batch_num + 1}/{total_batches}...\")\n",
        "    batch_df = df.iloc[batch_num * BATCH_SIZE : (batch_num + 1) * BATCH_SIZE].copy()\n",
        "    rows = list(batch_df[\"business_website\"].items())\n",
        "\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        for res in ex.map(scrape_email, rows):\n",
        "            results.append(res)\n",
        "\n",
        "    for rel_idx, email in results:\n",
        "        if isinstance(email, str):\n",
        "            batch_df.at[rel_idx, \"email\"] = email\n",
        "\n",
        "    out_path = f\"{OUTPUT_DIR}/batch_{batch_num + 1}.csv\"\n",
        "    batch_df.to_csv(out_path, index=False)\n",
        "    print(f\"✅ Saved {out_path} | found {(batch_df['email']!='').sum()}\")\n",
        "\n",
        "    all_batches.append(batch_df)\n",
        "\n",
        "combined = pd.concat(all_batches, ignore_index=True)\n",
        "combined_out = \"/content/emails_combined_fast.csv\"\n",
        "combined.to_csv(combined_out, index=False)\n",
        "print(f\"\\n🎉 Done. Combined CSV: {combined_out}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "041ke9xaSQW7",
        "outputId": "6a86a290-290f-4608-a35c-a78af645d954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your Bright Data API key (hidden): ··········\n",
            "Total rows: 359 | Batches: 4\n",
            "\n",
            "🚀 Processing batch 1/4...\n",
            "✅ Saved email_batches_fast/batch_1.csv | found 19\n",
            "\n",
            "🚀 Processing batch 2/4...\n",
            "✅ Saved email_batches_fast/batch_2.csv | found 21\n",
            "\n",
            "🚀 Processing batch 3/4...\n",
            "✅ Saved email_batches_fast/batch_3.csv | found 18\n",
            "\n",
            "🚀 Processing batch 4/4...\n",
            "✅ Saved email_batches_fast/batch_4.csv | found 12\n",
            "\n",
            "🎉 Done. Combined CSV: /content/emails_combined_fast.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Older version with ChatGPT 4.0"
      ],
      "metadata": {
        "id": "qXo1BsqN0CdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting email from the websites\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import math\n",
        "\n",
        "# ----------------- CONFIGURATION -----------------\n",
        "BRIGHT_DATA_API_KEY = \"bc218b6f2fa1fa2a24f87485659ced353e40b344270c5de00130562eb33f4f81\"  # 🔁 Replace with your Bright Data API key\n",
        "ZONE_NAME = \"clutch\"                 # 🔁 Replace with your zone name\n",
        "CSV_FILE = \"/content/limousine_brightdata_cleaned.csv\"\n",
        "OUTPUT_DIR = \"email_batches_1\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "BATCH_SIZE = 100\n",
        "MAX_WORKERS = 20\n",
        "# -------------------------------------------------\n",
        "\n",
        "API_URL = \"https://api.brightdata.com/request\"\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {BRIGHT_DATA_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "def extract_first_email(text):\n",
        "    for match in re.finditer(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.(?:com|org|net|edu)\", text):\n",
        "        email = match.group(0)\n",
        "        if not email.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "            return email\n",
        "    return \"\"\n",
        "\n",
        "def find_contact_url(html, base_url):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    base_domain = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(base_url))\n",
        "    contact_keywords = [\"contact\", \"contact-us\", \"contactus\",\"contact us\"]\n",
        "    ignore_keywords = [\"facebook\", \"linkedin\", \"instagram\", \".jpg\", \".png\", \"mailto\"]\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a['href'].lower()\n",
        "        if any(k in href for k in contact_keywords) and not any(b in href for b in ignore_keywords):\n",
        "            return urljoin(base_domain, href)\n",
        "    return None\n",
        "\n",
        "def scrape_email(row):\n",
        "    idx, site = row\n",
        "    try:\n",
        "        data = {\n",
        "            \"zone\": ZONE_NAME,\n",
        "            \"url\": site,\n",
        "            \"format\": \"raw\"\n",
        "        }\n",
        "\n",
        "        # Homepage\n",
        "        res = requests.post(API_URL, headers=HEADERS, json=data, timeout=10)\n",
        "        # print(f\"Status: {res.status_code}, Length: {len(res.text)}\")\n",
        "        html = res.text\n",
        "        email = extract_first_email(html)\n",
        "        if email:\n",
        "            return idx, email\n",
        "\n",
        "        # Contact fallback\n",
        "        contact_url = find_contact_url(html, site)\n",
        "        if contact_url:\n",
        "            data[\"url\"] = contact_url\n",
        "            res2 = requests.post(API_URL, headers=HEADERS, json=data, timeout=10)\n",
        "            contact_html = res2.text\n",
        "            email = extract_first_email(contact_html)\n",
        "            return idx, email if email else \"\"\n",
        "    except:\n",
        "        pass\n",
        "    return idx, \"\"\n",
        "\n",
        "# Load and clean CSV\n",
        "df = pd.read_csv(CSV_FILE).dropna(subset=[\"website\"]).reset_index(drop=True)\n",
        "df = df[df['website'].str.lower().str.startswith(\"http\")]  # Filter invalid URLs\n",
        "# df = df.head(10)  # 🔁 Limit to first 10 rows for quick testing\n",
        "\n",
        "df[\"email\"] = \"\"\n",
        "\n",
        "# Batch Processing\n",
        "total_batches = math.ceil(len(df) / BATCH_SIZE)\n",
        "\n",
        "for batch_num in range(total_batches):\n",
        "    print(f\"🚀 Processing batch {batch_num + 1}/{total_batches}...\")\n",
        "\n",
        "    batch_df = df.iloc[batch_num * BATCH_SIZE:(batch_num + 1) * BATCH_SIZE].copy()\n",
        "    batch_rows = list(batch_df[\"website\"].items())\n",
        "\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        for result in executor.map(scrape_email, batch_rows):\n",
        "            results.append(result)\n",
        "\n",
        "    # Update emails\n",
        "    for rel_idx, email in results:\n",
        "        batch_df.at[rel_idx, \"email\"] = email\n",
        "\n",
        "    # Save batch result\n",
        "    output_file = f\"{OUTPUT_DIR}/batch_{batch_num + 1}.csv\"\n",
        "    batch_df.to_csv(output_file, index=False)\n",
        "    print(f\"✅ Saved {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "wfX20sw8kS8x",
        "outputId": "19c3503b-e3db-4a59-a490-5875306c0250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/limousine_brightdata_cleaned.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2468358294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Load and clean CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"website\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'website'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Filter invalid URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# df = df.head(10)  # 🔁 Limit to first 10 rows for quick testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/limousine_brightdata_cleaned.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "OUTPUT_DIR = \"email_batches_1\"\n",
        "all_files = glob.glob(f\"{OUTPUT_DIR}/batch_*.csv\")\n",
        "combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
        "combined_df.to_csv(\"all_emails_combined.csv\", index=False)\n",
        "print(\"✅ Combined all emails into all_emails_combined.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL-76rhonLHX",
        "outputId": "914b4884-4827-4892-a2fc-2933d8ed2fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Combined all emails into all_emails_combined.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter empty email column\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV\n",
        "df = pd.read_csv(\"/content/all_emails_combined.csv\")\n",
        "\n",
        "# Keep only rows where 'email' is null/empty\n",
        "no_email_df = df[df['email'].isna() | (df['email'].str.strip() == '')]\n",
        "\n",
        "# Save to a new CSV\n",
        "no_email_df.to_csv(\"no_emails.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Saved {len(no_email_df)} rows without emails to no_emails.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--aowjJh25R2",
        "outputId": "7936612e-e04f-4341-8043-5031e189dcb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 260 rows without emails to no_emails.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter not empty email column\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV\n",
        "df = pd.read_csv(\"/content/all_emails_combined.csv\")\n",
        "\n",
        "# Keep only rows where 'email' is NOT null/empty\n",
        "filtered_df = df[df['email'].notna() & (df['email'].str.strip() != '')]\n",
        "\n",
        "# Save to a new CSV\n",
        "filtered_df.to_csv(\"filtered_with_emails.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Saved {len(filtered_df)} rows to filtered_with_emails.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szAnfwv_7A8j",
        "outputId": "9f7a87ba-2760-47bd-f149-f220d02c6386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 170 rows to filtered_with_emails.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine two csv files\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "file1 = \"/content/emails_combined_fast.csv\"\n",
        "file2 = \"/content/filtered_with_emails_1.csv\"\n",
        "output_file = \"combined.csv\"\n",
        "\n",
        "# Read both CSVs\n",
        "df1 = pd.read_csv(file1)\n",
        "df2 = pd.read_csv(file2)\n",
        "\n",
        "# Combine them\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# (Optional) Remove duplicate rows based on all columns\n",
        "# combined_df = combined_df.drop_duplicates()\n",
        "\n",
        "# Save to new CSV\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"✅ Combined CSV saved as {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJKi62G_8l9j",
        "outputId": "a66cc45b-dbe2-41c4-a79a-c340ccdb6b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Combined CSV saved as combined.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee35178e"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "OUTPUT_DIR = \"email_batches_1\" # Make sure this matches your output directory\n",
        "\n",
        "# List all batch files\n",
        "all_batch_files = glob.glob(f\"{OUTPUT_DIR}/batch_*.csv\")\n",
        "print(f\"Found {len(all_batch_files)} batch files initially.\")\n",
        "\n",
        "# Define the files you want to KEEP\n",
        "files_to_keep = [\n",
        "    os.path.join(OUTPUT_DIR, \"batch_13.csv\"),\n",
        "    os.path.join(OUTPUT_DIR, \"batch_14.csv\"),\n",
        "    # Add any other files you want to keep here\n",
        "]\n",
        "\n",
        "deleted_count = 0\n",
        "# Iterate through all batch files and delete the ones not in the keep list\n",
        "for file_path in all_batch_files:\n",
        "    if file_path not in files_to_keep:\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "            print(f\"Deleted: {file_path}\")\n",
        "            deleted_count += 1\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(f\"\\nFinished deleting files. {deleted_count} files were deleted.\")\n",
        "print(f\"{len(glob.glob(f'{OUTPUT_DIR}/batch_*.csv'))} files remaining in {OUTPUT_DIR}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "all_files = glob.glob(f\"{OUTPUT_DIR}/batch_*.csv\")\n",
        "combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
        "combined_df.to_csv(\"all_emails_combined.csv\", index=False)\n",
        "print(\"✅ Combined all emails into all_emails_combined.csv\")\n"
      ],
      "metadata": {
        "id": "Xvb6WthvvRLJ",
        "outputId": "4e79be8d-c46d-4f28-b0a5-ed6909725509",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Combined all emails into all_emails_combined.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}